{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian_Pines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.metrics\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding3D, BatchNormalization, Flatten, Conv3D, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_class(Class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    indx_class = np.where(Gt == Class)\n",
    "    all_indx = [[indx_class[0][i], indx_class[1][i]] for i in range(len(indx_class[0])) if\n",
    "                len(Gt) - np.ceil(Cube_size / 2) > indx_class[0][i] > np.ceil(Cube_size / 2) and len(Gt[0]) - np.ceil(\n",
    "                    Cube_size / 2) > indx_class[1][i] > np.ceil(Cube_size / 2)]\n",
    "    lst = []\n",
    "    small_segmented_1.append(np.array(Data[:ch, all_indx[0][0] - int(Cube_size / 2):all_indx[0][0] + int(Cube_size / 2),\n",
    "                                      (all_indx[0][1] - int(Cube_size / 2)):all_indx[0][1] + int(Cube_size / 2)]))\n",
    "    small_seg_gt_1.append(Class)\n",
    "    lst.append([all_indx[0][0], all_indx[0][1]])\n",
    "    for i in range(1, len(all_indx)):\n",
    "        dist = []\n",
    "        for k in range(len(lst)):\n",
    "            d = math.sqrt((all_indx[i][0] - lst[k][0]) ** 2 + (all_indx[i][1] - lst[k][1]) ** 2)\n",
    "            dist.append(d)\n",
    "        if np.min(dist) > int(Cube_size * (1 - overlap_ratio)):\n",
    "            small_segmented_1.append(np.array(\n",
    "                Data[:ch, all_indx[i][0] - int(Cube_size / 2):all_indx[i][0] + int(Cube_size / 2),\n",
    "                (all_indx[i][1] - int(Cube_size / 2)):all_indx[i][1] + int(Cube_size / 2)]))\n",
    "            small_seg_gt_1.append(Class)\n",
    "            lst.append([all_indx[i][0], all_indx[i][1]])\n",
    "    return small_segmented_1, small_seg_gt_1, lst\n",
    "\n",
    "\n",
    "def pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    class_len = []\n",
    "    for i in range_of_class:\n",
    "        small_segmented_1, small_seg_gt_1, lst = pick_class(i, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1,\n",
    "                                                            overlap_ratio, ch)\n",
    "        class_len.append(len(lst))\n",
    "    small_segmented_1 = np.array(small_segmented_1)\n",
    "    small_seg_gt_1 = np.array(small_seg_gt_1)\n",
    "    return small_segmented_1, small_seg_gt_1, class_len\n",
    "\n",
    "\n",
    "def train_test_split(percentage, class_len, Data, Gt):\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    Ytrain = []\n",
    "    Ytest = []\n",
    "    class_division = [0]\n",
    "    c = 0\n",
    "    for i in range(len(class_len)):\n",
    "        class_division.append(int(class_len[i] * (percentage / 100)) + c)\n",
    "        class_division.append(class_len[i] + c)\n",
    "        c = class_len[i] + c\n",
    "    for i in range(1, len(class_division)):\n",
    "        if i % 2 != 0:\n",
    "            for j in range(class_division[i - 1], class_division[i]):\n",
    "                Xtrain.append(Data[j])\n",
    "                Ytrain.append(Gt[j])\n",
    "        else:\n",
    "            for k in range(class_division[i - 1], class_division[i]):\n",
    "                Xtest.append(Data[k])\n",
    "                Ytest.append(Gt[k])\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xtest = np.array(Xtest)\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    Ytest = np.array(Ytest)\n",
    "    s = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtrain = Xtrain[s]\n",
    "    Ytrain = Ytrain[s]\n",
    "    s = np.arange(Xtest.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtest = Xtest[s]\n",
    "    Ytest = Ytest[s]\n",
    "    Xtrain = np.expand_dims(Xtrain, axis=4)\n",
    "    Xtest = np.expand_dims(Xtest, axis=4)\n",
    "    Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape\n",
    "    values, counts = np.unique(Ytest, return_counts=True)\n",
    "    print(\n",
    "        \"Total samples per class: \" + str(class_len) + \", Total number of samples is \" + str(np.sum(class_len)) + '.\\n')\n",
    "    print(\"unique classes in Ytest: \" + str(values) + \", Total number of samples in Ytest is \" + str(\n",
    "        np.sum(counts)) + '.\\n'\n",
    "          + \"number of samples per class in Ytest: \" + str(counts) + '\\n')\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    Ytrain = Ytrain.reshape(len(Ytrain), 1)\n",
    "    Ytest = Ytest.reshape(len(Ytest), 1)\n",
    "    Ytrain = onehot_encoder.fit_transform(Ytrain)\n",
    "    Ytest = onehot_encoder.fit_transform(Ytest)\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "\n",
    "def prepare_data_for_training(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, percentage,\n",
    "                              overlap_ratio, ch):\n",
    "    small_segmented_1, small_seg_gt_1, class_len = pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1,\n",
    "                                                                small_seg_gt_1, overlap_ratio, ch)\n",
    "    Xtrain, Xtest, Ytrain, Ytest, class_len, counts = train_test_split(percentage, class_len, small_segmented_1,\n",
    "                                                                       small_seg_gt_1)\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "\n",
    "def SR_Unit(X, filters):\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    l2_ = 0.01\n",
    "    X = Conv3D(filters, (1, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_1')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_1')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_1')(X)\n",
    "\n",
    "    X = Conv3D(filters, kernel_size=(1, 3, 3), kernel_regularizer=regularizers.l2(l2_), padding='same',\n",
    "               name='SR_Unit_' + str(filters) + '_conv_2')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_2')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_2')(X)\n",
    "\n",
    "    X = Conv3D(filters, kernel_size=(3, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_3')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_3')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_3')(X)\n",
    "\n",
    "    X = Conv3D(2 * filters, kernel_size=(1, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_4')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_4')(X)\n",
    "\n",
    "    X_shortcut = Conv3D(filters=2 * filters, kernel_size=(3, 3, 3), padding='same',\n",
    "                        kernel_regularizer=regularizers.l2(l2_), name='SR_Unit_' + str(filters) + '_conv_5')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_5')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_5')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def feature_extraction(Sample):\n",
    "    X = Sample\n",
    "\n",
    "    X = Conv3D(32, (8, 3, 3), kernel_regularizer=regularizers.l2(0.01), strides=(2, 2, 2), name='conv_0')(X)\n",
    "    X = BatchNormalization(axis=4, name='bn_conv_0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling3D((1, 2, 2), strides=None, name='MaxPooling_0')(X)\n",
    "\n",
    "    # Stage 2\n",
    "    F1 = 16\n",
    "    X = SR_Unit(X, filters=F1)\n",
    "\n",
    "    F2 = 32\n",
    "    X = SR_Unit(X, filters=F2)\n",
    "\n",
    "    #     F3 = 64\n",
    "    #     X = SR_Unit(X, filters=F3)\n",
    "\n",
    "    #     F4 = 128\n",
    "    #     X = SR_Unit(X, filters=F4)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def model(input_shape, classes):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = feature_extraction(X_input)\n",
    "    X = GlobalMaxPooling3D()(X)\n",
    "    X = Dense(256, input_dim=X.shape, activation='relu', name='fc_256', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, input_dim=X.shape, activation='softmax', name='fc' + str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"3D-SRNet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uIndian_pines = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Indian Pines\\\\Indian_pines_corrected.mat')\n",
    "gt_uIndian_pines = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Indian Pines\\\\Indian_pines_gt.mat')\n",
    "data_IN = uIndian_pines['indian_pines_corrected']\n",
    "gt_IN = gt_uIndian_pines['indian_pines_gt']\n",
    "print(data_IN.shape, gt_IN.shape)\n",
    "data_IN = np.moveaxis(data_IN, 2, 0)\n",
    "print(data_IN.shape, gt_IN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,counts = np.unique(gt_IN, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_ratio = 0.90\n",
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class = range_of_class, Cube_size = 10, Data = data_IN , Gt = gt_IN, small_segmented_1 = [], small_seg_gt_1 = [], percentage = 80, overlap_ratio=overlap_ratio, ch=103)\n",
    "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model(input_shape = Xtrain[0].shape, classes = len(range_of_class))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('Trained Models\\\\model_full_best_pavia_overlap_ratio_'+ str(int(overlap_ratio*100)) + '_percent.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True)\n",
    "model_1.compile(optimizer= keras.optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain, Ytrain, epochs = 500, batch_size = 16, validation_data=(Xtest,Ytest), verbose=1, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_1.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1._layers.pop()\n",
    "model_1._layers.pop()\n",
    "# last_layer = model_1._layers.pop()\n",
    "# second_last_layer = model_1._layers.pop()\n",
    "model_2 =  Model(model_1.inputs, model_1.layers[-1].output)\n",
    "model_2.compile(optimizer=keras.optimizers.SGD(lr=0.00001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_2.set_weights(model_1.get_weights())\n",
    "model_2.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('..\\\\..\\\\Trained Models\\\\Sub_Model\\\\Sub_model_Transfer_'+ data_name +'_overlap_ratio_' + str(int(overlap_ratio * 100)) + '_percent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1._layers.pop()\n",
    "model_1._layers.pop()\n",
    "# last_layer = model_1._layers.pop()\n",
    "# second_last_layer = model_1._layers.pop()\n",
    "model_2 =  Model(model_1.inputs, model_1.layers[-1].output)\n",
    "model_2.compile(optimizer=keras.optimizers.SGD(lr=0.00001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_2.set_weights(model_1.get_weights())\n",
    "model_2.summary() \n",
    "\n",
    "model_2.save('..\\\\..\\\\Trained Models\\\\Sub_Model\\\\Sub_model_Transfer_'+ data_name +'_overlap_ratio_' + str(int(overlap_ratio * 100)) + '_percent.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

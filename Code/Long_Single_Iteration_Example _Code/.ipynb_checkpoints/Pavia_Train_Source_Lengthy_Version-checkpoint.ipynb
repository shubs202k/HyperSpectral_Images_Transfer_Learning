{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pavia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.metrics\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding3D, BatchNormalization, Flatten, Conv3D, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_class(Class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    indx_class = np.where(Gt == Class)\n",
    "    all_indx = [[indx_class[0][i], indx_class[1][i]] for i in range(len(indx_class[0])) if\n",
    "                len(Gt) - np.ceil(Cube_size / 2) > indx_class[0][i] > np.ceil(Cube_size / 2) and len(Gt[0]) - np.ceil(\n",
    "                    Cube_size / 2) > indx_class[1][i] > np.ceil(Cube_size / 2)]\n",
    "    lst = []\n",
    "    small_segmented_1.append(np.array(Data[:ch, all_indx[0][0] - int(Cube_size / 2):all_indx[0][0] + int(Cube_size / 2),\n",
    "                                      (all_indx[0][1] - int(Cube_size / 2)):all_indx[0][1] + int(Cube_size / 2)]))\n",
    "    small_seg_gt_1.append(Class)\n",
    "    lst.append([all_indx[0][0], all_indx[0][1]])\n",
    "    for i in range(1, len(all_indx)):\n",
    "        dist = []\n",
    "        for k in range(len(lst)):\n",
    "            d = math.sqrt((all_indx[i][0] - lst[k][0]) ** 2 + (all_indx[i][1] - lst[k][1]) ** 2)\n",
    "            dist.append(d)\n",
    "        if np.min(dist) > int(Cube_size * (1 - overlap_ratio)):\n",
    "            small_segmented_1.append(np.array(\n",
    "                Data[:ch, all_indx[i][0] - int(Cube_size / 2):all_indx[i][0] + int(Cube_size / 2),\n",
    "                (all_indx[i][1] - int(Cube_size / 2)):all_indx[i][1] + int(Cube_size / 2)]))\n",
    "            small_seg_gt_1.append(Class)\n",
    "            lst.append([all_indx[i][0], all_indx[i][1]])\n",
    "    return small_segmented_1, small_seg_gt_1, lst\n",
    "\n",
    "\n",
    "def pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    class_len = []\n",
    "    for i in range_of_class:\n",
    "        small_segmented_1, small_seg_gt_1, lst = pick_class(i, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1,\n",
    "                                                            overlap_ratio, ch)\n",
    "        class_len.append(len(lst))\n",
    "    small_segmented_1 = np.array(small_segmented_1)\n",
    "    small_seg_gt_1 = np.array(small_seg_gt_1)\n",
    "    return small_segmented_1, small_seg_gt_1, class_len\n",
    "\n",
    "\n",
    "def train_test_split(percentage, class_len, Data, Gt):\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    Ytrain = []\n",
    "    Ytest = []\n",
    "    class_division = [0]\n",
    "    c = 0\n",
    "    for i in range(len(class_len)):\n",
    "        class_division.append(int(class_len[i] * (percentage / 100)) + c)\n",
    "        class_division.append(class_len[i] + c)\n",
    "        c = class_len[i] + c\n",
    "    for i in range(1, len(class_division)):\n",
    "        if i % 2 != 0:\n",
    "            for j in range(class_division[i - 1], class_division[i]):\n",
    "                Xtrain.append(Data[j])\n",
    "                Ytrain.append(Gt[j])\n",
    "        else:\n",
    "            for k in range(class_division[i - 1], class_division[i]):\n",
    "                Xtest.append(Data[k])\n",
    "                Ytest.append(Gt[k])\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xtest = np.array(Xtest)\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    Ytest = np.array(Ytest)\n",
    "    s = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtrain = Xtrain[s]\n",
    "    Ytrain = Ytrain[s]\n",
    "    s = np.arange(Xtest.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtest = Xtest[s]\n",
    "    Ytest = Ytest[s]\n",
    "    Xtrain = np.expand_dims(Xtrain, axis=4)\n",
    "    Xtest = np.expand_dims(Xtest, axis=4)\n",
    "    Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape\n",
    "    values, counts = np.unique(Ytest, return_counts=True)\n",
    "    print(\n",
    "        \"Total samples per class: \" + str(class_len) + \", Total number of samples is \" + str(np.sum(class_len)) + '.\\n')\n",
    "    print(\"unique classes in Ytest: \" + str(values) + \", Total number of samples in Ytest is \" + str(\n",
    "        np.sum(counts)) + '.\\n'\n",
    "          + \"number of samples per class in Ytest: \" + str(counts) + '\\n')\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    Ytrain = Ytrain.reshape(len(Ytrain), 1)\n",
    "    Ytest = Ytest.reshape(len(Ytest), 1)\n",
    "    Ytrain = onehot_encoder.fit_transform(Ytrain)\n",
    "    Ytest = onehot_encoder.fit_transform(Ytest)\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "\n",
    "def prepare_data_for_training(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, percentage,\n",
    "                              overlap_ratio, ch):\n",
    "    small_segmented_1, small_seg_gt_1, class_len = pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1,\n",
    "                                                                small_seg_gt_1, overlap_ratio, ch)\n",
    "    Xtrain, Xtest, Ytrain, Ytest, class_len, counts = train_test_split(percentage, class_len, small_segmented_1,\n",
    "                                                                       small_seg_gt_1)\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "\n",
    "def SR_Unit(X, filters):\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    l2_ = 0.01\n",
    "    X = Conv3D(filters, (1, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_1')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_1')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_1')(X)\n",
    "\n",
    "    X = Conv3D(filters, kernel_size=(1, 3, 3), kernel_regularizer=regularizers.l2(l2_), padding='same',\n",
    "               name='SR_Unit_' + str(filters) + '_conv_2')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_2')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_2')(X)\n",
    "\n",
    "    X = Conv3D(filters, kernel_size=(3, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_3')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_3')(X)\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_3')(X)\n",
    "\n",
    "    X = Conv3D(2 * filters, kernel_size=(1, 1, 1), kernel_regularizer=regularizers.l2(l2_), padding=\"same\",\n",
    "               name='SR_Unit_' + str(filters) + '_conv_4')(X)\n",
    "    X = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_4')(X)\n",
    "\n",
    "    X_shortcut = Conv3D(filters=2 * filters, kernel_size=(3, 3, 3), padding='same',\n",
    "                        kernel_regularizer=regularizers.l2(l2_), name='SR_Unit_' + str(filters) + '_conv_5')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=4, name='SR_Unit_' + str(filters) + '_bn_5')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu', name='SR_Unit_' + str(filters) + '_activation_5')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def feature_extraction(Sample):\n",
    "    X = Sample\n",
    "\n",
    "    X = Conv3D(32, (8, 3, 3), kernel_regularizer=regularizers.l2(0.01), strides=(2, 2, 2), name='conv_0')(X)\n",
    "    X = BatchNormalization(axis=4, name='bn_conv_0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling3D((1, 2, 2), strides=None, name='MaxPooling_0')(X)\n",
    "\n",
    "    # Stage 2\n",
    "    F1 = 16\n",
    "    X = SR_Unit(X, filters=F1)\n",
    "\n",
    "    F2 = 32\n",
    "    X = SR_Unit(X, filters=F2)\n",
    "\n",
    "    #     F3 = 64\n",
    "    #     X = SR_Unit(X, filters=F3)\n",
    "\n",
    "    #     F4 = 128\n",
    "    #     X = SR_Unit(X, filters=F4)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def model(input_shape, classes):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = feature_extraction(X_input)\n",
    "    X = GlobalMaxPooling3D()(X)\n",
    "    X = Dense(256, input_dim=X.shape, activation='relu', name='fc_256', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, input_dim=X.shape, activation='softmax', name='fc' + str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"3D-SRNet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 340, 103) (610, 340)\n",
      "(103, 610, 340) (610, 340)\n"
     ]
    }
   ],
   "source": [
    "uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU.mat')\n",
    "gt_uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU_gt.mat')\n",
    "data_PV = uPavia['paviaU']\n",
    "gt_PV = gt_uPavia['paviaU_gt']\n",
    "print(data_PV.shape, gt_PV.shape)\n",
    "data_PV = np.moveaxis(data_PV, 2, 0)\n",
    "print(data_PV.shape, gt_PV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] [164624   6631  18649   2099   3064   1345   5029   1330   3682    947]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,counts = np.unique(gt_PV, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples per class: [74, 72, 19, 98, 12, 19, 10, 56, 31], Total number of samples is 391.\n",
      "\n",
      "unique classes in Ytest: [1 2 3 4 5 6 7 8 9], Total number of samples in Ytest is 82.\n",
      "number of samples per class in Ytest: [15 15  4 20  3  4  2 12  7]\n",
      "\n",
      "(309, 103, 24, 24, 1) (82, 103, 24, 24, 1) (309, 9) (82, 9)\n"
     ]
    }
   ],
   "source": [
    "overlap_ratio = 0.25\n",
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class = range_of_class, Cube_size = 25, Data = data_PV , Gt = gt_PV, small_segmented_1 = [], small_seg_gt_1 = [], percentage = 80, overlap_ratio = overlap_ratio, ch = 103)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 103, 24, 24,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv3D)                 (None, 48, 11, 11, 3 2336        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv_0 (BatchNormalizationV1 (None, 48, 11, 11, 3 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 48, 11, 11, 3 0           bn_conv_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_0 (MaxPooling3D)     (None, 48, 5, 5, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_1 (Conv3D)      (None, 48, 5, 5, 16) 528         MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_1 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_1 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_2 (Conv3D)      (None, 48, 5, 5, 16) 2320        SR_Unit_16_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_2 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_2 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_3 (Conv3D)      (None, 48, 5, 5, 16) 784         SR_Unit_16_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_3 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_3 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_4 (Conv3D)      (None, 48, 5, 5, 32) 544         SR_Unit_16_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_5 (Conv3D)      (None, 48, 5, 5, 32) 27680       MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_4 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_5 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 48, 5, 5, 32) 0           SR_Unit_16_bn_4[0][0]            \n",
      "                                                                 SR_Unit_16_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_5 (Activa (None, 48, 5, 5, 32) 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_1 (Conv3D)      (None, 48, 5, 5, 32) 1056        SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_1 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_1 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_2 (Conv3D)      (None, 48, 5, 5, 32) 9248        SR_Unit_32_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_2 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_2 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_3 (Conv3D)      (None, 48, 5, 5, 32) 3104        SR_Unit_32_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_3 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_3 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_4 (Conv3D)      (None, 48, 5, 5, 64) 2112        SR_Unit_32_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_5 (Conv3D)      (None, 48, 5, 5, 64) 55360       SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_4 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_5 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 48, 5, 5, 64) 0           SR_Unit_32_bn_4[0][0]            \n",
      "                                                                 SR_Unit_32_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_5 (Activa (None, 48, 5, 5, 64) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling3d (GlobalMax (None, 64)           0           SR_Unit_32_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "fc_256 (Dense)                  (None, 256)          16640       global_max_pooling3d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fc9 (Dense)                     (None, 9)            2313        fc_256[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 125,497\n",
      "Trainable params: 124,761\n",
      "Non-trainable params: 736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = model(input_shape = Xtrain[0].shape, classes = len(range_of_class))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 309 samples, validate on 82 samples\n",
      "Epoch 1/3\n",
      "256/309 [=======================>......] - ETA: 3s - loss: 7.6687 - categorical_accuracy: 0.0234 \n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.03659, saving model to ..\\..\\Trained Models\\model_full_best_pavia_overlap_ratio_25_percent.h5\n",
      "309/309 [==============================] - 20s 64ms/sample - loss: 7.4258 - categorical_accuracy: 0.0291 - val_loss: 7.7829 - val_categorical_accuracy: 0.0366\n",
      "Epoch 2/3\n",
      "256/309 [=======================>......] - ETA: 2s - loss: 6.2053 - categorical_accuracy: 0.1367\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.03659 to 0.24390, saving model to ..\\..\\Trained Models\\model_full_best_pavia_overlap_ratio_25_percent.h5\n",
      "309/309 [==============================] - 15s 48ms/sample - loss: 6.1308 - categorical_accuracy: 0.1489 - val_loss: 6.1387 - val_categorical_accuracy: 0.2439\n",
      "Epoch 3/3\n",
      "256/309 [=======================>......] - ETA: 2s - loss: 5.5952 - categorical_accuracy: 0.2422\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.24390\n",
      "309/309 [==============================] - 15s 48ms/sample - loss: 5.6018 - categorical_accuracy: 0.2557 - val_loss: 6.0641 - val_categorical_accuracy: 0.2439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b16b6e2ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('..\\\\..\\\\Trained Models\\\\model_full_best_pavia_overlap_ratio_'+ str(int(overlap_ratio*100)) + '_percent.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True)\n",
    "model_1.compile(optimizer= keras.optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain, Ytrain, epochs = 3, batch_size = 128, validation_data=(Xtest,Ytest), verbose=1, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 15ms/sample - loss: 6.0641 - categorical_accuracy: 0.2439\n",
      "Test Accuracy = 0.24390244\n"
     ]
    }
   ],
   "source": [
    "preds = model_1.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 2s 19ms/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_1.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, 15,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 15,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 20,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 12,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  7,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 15,  4, 20,  3,  4,  2, 12,  7], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7d0c3ebd4228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# last_layer = model_1._layers.pop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# second_last_layer = model_1._layers.pop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_2\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "model_1._layers.pop()\n",
    "model_1._layers.pop()\n",
    "# last_layer = model_1._layers.pop()\n",
    "# second_last_layer = model_1._layers.pop()\n",
    "model_2 =  Model(model_1.inputs, model_1.layers[-1].output)\n",
    "model_2.compile(optimizer=keras.optimizers.SGD(lr=0.00001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_2.set_weights(model_1.get_weights())\n",
    "model_2.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('..\\\\..\\\\Trained Models\\\\Sub_Model\\\\Sub_model_Transfer_'+ data_name +'_overlap_ratio_' + str(int(overlap_ratio * 100)) + '_percent.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

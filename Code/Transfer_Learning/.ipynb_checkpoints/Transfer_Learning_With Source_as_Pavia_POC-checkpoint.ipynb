{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(overlap_ratio_List, range_of_class, Cube_size,  Data, Gt, Train_Test_split, channel, \n",
    "          epochs_list, batch_size_list, Source_data_name, Target_data_name, Source_Model):\n",
    "    \n",
    "    \n",
    "    for i in range(len(overlap_ratio_List)):\n",
    "        print(\"\\n\\n===============================================================================================================================\\n\"\n",
    "              \"======================================== Data with overlap ratio of \"+ str(overlap_ratio_List[i]) + \" will be trained ========================================\\n\"\n",
    "              \"===============================================================================================================================\\n\\n\")\n",
    "        overlap_ratio = overlap_ratio_List[i] / 100\n",
    "        Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class=range_of_class,\n",
    "                                                                                    Cube_size=Cube_size, Data=Data,\n",
    "                                                                                    Gt=Gt, small_segmented_1=[],\n",
    "                                                                                    small_seg_gt_1=[], percentage=Train_Test_split,\n",
    "                                                                                    overlap_ratio=overlap_ratio, ch=channel)\n",
    "        \n",
    "        \n",
    "        \n",
    "        Xtrain_transfer = Source_Model.predict(Xtrain)\n",
    "        Xtest_transfer = Source_Model.predict(Xtest)\n",
    "        Xtrain_transfer = np.array(Xtrain_transfer)\n",
    "        Xtest_transfer = np.array(Xtest_transfer)\n",
    "        print('Xtrain => ' + str(Xtrain.shape) + '\\n' +\n",
    "              'Xtest  => ' + str(Xtest.shape) + '\\n' +\n",
    "              'Ytrain => ' + str(Ytrain.shape) + '\\n' +\n",
    "              'Ytest  => ' + str(Ytest.shape) + '\\n')\n",
    "        model_1 = model_transfer(input_shape=Xtrain[0].shape, classes=len(range_of_class))\n",
    "        model_1.summary()\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint('..\\\\..\\\\Trained Models\\\\Transfered_Models\\\\Fully_Connected_Layers_Only\\\\'\n",
    "                                           +Target_data_name+'\\\\Fully_Connected_Layers_for_'+ Fully_Connected_Layers_Only +\n",
    "                                           '_With_source_data_as_'+ Fully_Connected_Layers_Only +'_overlap_ratio_' + \n",
    "                                           str(int(overlap_ratio * 100)) + '_percent.h5',\n",
    "                                           monitor='val_categorical_accuracy', verbose=1, save_best_only=True)\n",
    "        model_1.compile(optimizer=keras.optimizers.SGD(lr=0.00001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                        loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        model_1.fit(Xtrain, Ytrain, epochs=epochs_list[i], batch_size=batch_size_list[i], validation_data=(Xtest, Ytest), verbose=1,\n",
    "                    callbacks=[model_checkpoint])\n",
    "\n",
    "        preds = model_1.evaluate(Xtest, Ytest)\n",
    "        # print (\"Loss = \" + str(preds[0]))\n",
    "        print(\"Test Accuracy = \" + str(preds[1]))\n",
    "\n",
    "        y_pred = model_1.predict(Xtest, verbose=1)\n",
    "        confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "        print(\"\\n===============================================================================================================================\\n\"\n",
    "              \"=============================== Confusion_matrix for data with overlap ratio of  \"+ str(overlap_ratio_List[i]) + \" is as below ===============================\\n\"\n",
    "              \"===============================================================================================================================\\n\")\n",
    "        print(confusion_matrix)\n",
    "        print(\"==============================================================================================================================\")\n",
    "        print(counts)\n",
    "        print(\"==============================================================================================================================\\n\\n\"\n",
    "              \"==============================================================================================================================\\n\"\n",
    "              \"=============================== Sub Model below will be saved to be used for transfer learning  ==============================\\n\"\n",
    "              \"==============================================================================================================================\\n\")\n",
    "        model_1._layers.pop()\n",
    "        model_1._layers.pop()\n",
    "        # last_layer = model_1._layers.pop()\n",
    "        # second_last_layer = model_1._layers.pop()\n",
    "        model_2 =  Model(model_1.inputs, model_1.layers[-1].output)\n",
    "        model_2.compile(optimizer=keras.optimizers.SGD(lr=0.001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                        loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        model_2.set_weights(model_1.get_weights())\n",
    "        model_2.summary()\n",
    "\n",
    "        model_2.save('..\\\\..\\\\Trained Models\\\\Sub_Model\\\\'+data_name+'\\\\Sub_model_Transfer_'+ data_name +'_overlap_ratio_' + str(int(overlap_ratio * 100)) + '_percent.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transfer_Model_Utils import *\n",
    "from tensorflow.keras.models import load_model\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 103, 24, 24,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv3D)                 (None, 48, 11, 11, 3 2336        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv_0 (BatchNormalizationV1 (None, 48, 11, 11, 3 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 48, 11, 11, 3 0           bn_conv_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_0 (MaxPooling3D)     (None, 48, 5, 5, 32) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_1 (Conv3D)      (None, 48, 5, 5, 16) 528         MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_1 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_1 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_2 (Conv3D)      (None, 48, 5, 5, 16) 2320        SR_Unit_16_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_2 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_2 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_3 (Conv3D)      (None, 48, 5, 5, 16) 784         SR_Unit_16_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_3 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_3 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_4 (Conv3D)      (None, 48, 5, 5, 32) 544         SR_Unit_16_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_5 (Conv3D)      (None, 48, 5, 5, 32) 27680       MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_4 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_5 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 48, 5, 5, 32) 0           SR_Unit_16_bn_4[0][0]            \n",
      "                                                                 SR_Unit_16_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_5 (Activa (None, 48, 5, 5, 32) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_1 (Conv3D)      (None, 48, 5, 5, 32) 1056        SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_1 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_1 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_2 (Conv3D)      (None, 48, 5, 5, 32) 9248        SR_Unit_32_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_2 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_2 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_3 (Conv3D)      (None, 48, 5, 5, 32) 3104        SR_Unit_32_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_3 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_3 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_4 (Conv3D)      (None, 48, 5, 5, 64) 2112        SR_Unit_32_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_5 (Conv3D)      (None, 48, 5, 5, 64) 55360       SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_4 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_5 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 48, 5, 5, 64) 0           SR_Unit_32_bn_4[0][0]            \n",
      "                                                                 SR_Unit_32_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_5 (Activa (None, 48, 5, 5, 64) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling3d_1 (GlobalM (None, 64)           0           SR_Unit_32_activation_5[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 106,544\n",
      "Trainable params: 105,808\n",
      "Non-trainable params: 736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pavia_50 = load_model('..\\\\..\\\\Trained Models\\\\Sub_Model\\\\Pavia\\\\Sub_model_Transfer_Pavia_overlap_ratio_50_percent.h5')\n",
    "model_pavia_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 103, 24, 24,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv3D)                 (None, 48, 11, 11, 3 2336        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv_0 (BatchNormalizationV1 (None, 48, 11, 11, 3 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 48, 11, 11, 3 0           bn_conv_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling_0 (MaxPooling3D)     (None, 48, 5, 5, 32) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_1 (Conv3D)      (None, 48, 5, 5, 16) 528         MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_1 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_1 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_2 (Conv3D)      (None, 48, 5, 5, 16) 2320        SR_Unit_16_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_2 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_2 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_3 (Conv3D)      (None, 48, 5, 5, 16) 784         SR_Unit_16_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_3 (BatchNormaliza (None, 48, 5, 5, 16) 64          SR_Unit_16_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_3 (Activa (None, 48, 5, 5, 16) 0           SR_Unit_16_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_4 (Conv3D)      (None, 48, 5, 5, 32) 544         SR_Unit_16_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_conv_5 (Conv3D)      (None, 48, 5, 5, 32) 27680       MaxPooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_4 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_bn_5 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_16_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 48, 5, 5, 32) 0           SR_Unit_16_bn_4[0][0]            \n",
      "                                                                 SR_Unit_16_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_16_activation_5 (Activa (None, 48, 5, 5, 32) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_1 (Conv3D)      (None, 48, 5, 5, 32) 1056        SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_1 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_1 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_2 (Conv3D)      (None, 48, 5, 5, 32) 9248        SR_Unit_32_activation_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_2 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_2 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_3 (Conv3D)      (None, 48, 5, 5, 32) 3104        SR_Unit_32_activation_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_3 (BatchNormaliza (None, 48, 5, 5, 32) 128         SR_Unit_32_conv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_3 (Activa (None, 48, 5, 5, 32) 0           SR_Unit_32_bn_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_4 (Conv3D)      (None, 48, 5, 5, 64) 2112        SR_Unit_32_activation_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_conv_5 (Conv3D)      (None, 48, 5, 5, 64) 55360       SR_Unit_16_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_4 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_bn_5 (BatchNormaliza (None, 48, 5, 5, 64) 256         SR_Unit_32_conv_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 48, 5, 5, 64) 0           SR_Unit_32_bn_4[0][0]            \n",
      "                                                                 SR_Unit_32_bn_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "SR_Unit_32_activation_5 (Activa (None, 48, 5, 5, 64) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling3d_1 (GlobalM (None, 64)           0           SR_Unit_32_activation_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "fc_256 (Dense)                  (None, 256)          16640       global_max_pooling3d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc9 (Dense)                     (None, 9)            2313        fc_256[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 125,497\n",
      "Trainable params: 124,761\n",
      "Non-trainable params: 736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pavia_50_F = load_model('..\\\\..\\\\Trained Models\\\\Full_Model\\\\Pavia\\\\Full_Model_best_Pavia_overlap_ratio_50_percent.h5')\n",
    "model_pavia_50_F.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 217, 204) (512, 217)\n",
      "(204, 512, 217) (512, 217)\n"
     ]
    }
   ],
   "source": [
    "uSalinas = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Salinas\\\\Salinas_corrected.mat')\n",
    "gt_uSalinas = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Salinas\\\\Salinas_gt.mat')\n",
    "data_SA = uSalinas['salinas_corrected']\n",
    "gt_SA = gt_uSalinas['salinas_gt']\n",
    "print(data_SA.shape, gt_SA.shape)\n",
    "data_SA = np.moveaxis(data_SA, 2, 0)\n",
    "print(data_SA.shape, gt_SA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16] [56975  2009  3726  1976  1394  2678  3959  3579 11271  6203  3278  1068\n",
      "  1927   916  1070  7268  1807]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,counts = np.unique(gt_SA, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU.mat')\n",
    "gt_uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU_gt.mat')\n",
    "data_PV = uPavia['paviaU']\n",
    "gt_PV = gt_uPavia['paviaU_gt']\n",
    "print(data_PV.shape, gt_PV.shape)\n",
    "data_PV = np.moveaxis(data_PV, 2, 0)\n",
    "print(data_PV.shape, gt_PV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,counts = np.unique(gt_PV, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples per class: [15, 32, 17, 14, 26, 35, 30, 80, 51, 27, 10, 17, 9, 10, 47, 9], Total number of samples is 429.\n",
      "\n",
      "unique classes in Ytest: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16], Total number of samples in Ytest is 91.\n",
      "number of samples per class in Ytest: [ 3  7  4  3  6  7  6 16 11  6  2  4  2  2 10  2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overlap_ratio = 50 / 100\n",
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class=range_of_class,\n",
    "                                                                            Cube_size=25, Data=data_SA,\n",
    "                                                                            Gt=gt_SA, small_segmented_1=[],\n",
    "                                                                            small_seg_gt_1=[], percentage=80,\n",
    "                                                                            overlap_ratio=overlap_ratio, ch=103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain => (338, 103, 24, 24, 1)\n",
      "Xtest  => (91, 103, 24, 24, 1)\n",
      "Ytrain => (338, 16)\n",
      "Ytest  => (91, 16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Xtrain => ' + str(Xtrain.shape) + '\\n' +\n",
    "      'Xtest  => ' + str(Xtest.shape) + '\\n' +\n",
    "      'Ytrain => ' + str(Ytrain.shape) + '\\n' +\n",
    "      'Ytest  => ' + str(Ytest.shape) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (91, 16) was passed for an output of shape (None, 9) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9ba9e4c20323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_pavia_50_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# print (\"Loss = \" + str(preds[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Accuracy = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2438\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2439\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2440\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2441\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2442\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    511\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m                            \u001b[1;34m'This loss expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                            \u001b[1;34m'targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (91, 16) was passed for an output of shape (None, 9) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "preds = model_pavia_50_F.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 4s 46ms/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_pavia_50_F.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  3,  0,  2,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 9,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  4,  3,  6,  7,  6, 16, 11,  6,  2,  4,  2,  2, 10,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_transfer = model_pavia_50.predict(Xtrain)\n",
    "Xtest_transfer = model_pavia_50.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain => (338, 64)\n",
      "Xtest  => (91, 64)\n",
      "Ytrain => (338, 16)\n",
      "Ytest  => (91, 16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Xtrain => ' + str(Xtrain_transfer.shape) + '\\n' +\n",
    "      'Xtest  => ' + str(Xtest_transfer.shape) + '\\n' +\n",
    "      'Ytrain => ' + str(Ytrain.shape) + '\\n' +\n",
    "      'Ytest  => ' + str(Ytest.shape) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_transfer(input_shape=(64), classes=9):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = Dense(256, input_dim=X_input.shape, activation='relu', name='fc_256',\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
    "    X = Dense(classes, input_dim=X.shape, activation='softmax', name='fc' + str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"model_transfer\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc_256 (Dense)               (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "fc16 (Dense)                 (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 20,752\n",
      "Trainable params: 20,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = model_transfer(input_shape=Xtrain_transfer[0].shape, classes=len(range_of_class))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_data_name= 'Pavia44'\n",
    "Target_data_name='Pavia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 338 samples, validate on 91 samples\n",
      "Epoch 1/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.7146 - categorical_accuracy: 0.7955\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.63736, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 2s 7ms/sample - loss: 1.6087 - categorical_accuracy: 0.7811 - val_loss: 1.8526 - val_categorical_accuracy: 0.6374\n",
      "Epoch 2/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.4546 - categorical_accuracy: 0.8333\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.63736\n",
      "338/338 [==============================] - 0s 263us/sample - loss: 1.6129 - categorical_accuracy: 0.8047 - val_loss: 1.8569 - val_categorical_accuracy: 0.5934\n",
      "Epoch 3/300\n",
      "144/338 [===========>..................] - ETA: 0s - loss: 1.9853 - categorical_accuracy: 0.7847\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.63736 to 0.65934, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 0s 379us/sample - loss: 1.5988 - categorical_accuracy: 0.7899 - val_loss: 1.8327 - val_categorical_accuracy: 0.6593\n",
      "Epoch 4/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.3613 - categorical_accuracy: 0.7969\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 619us/sample - loss: 1.6067 - categorical_accuracy: 0.7811 - val_loss: 1.8350 - val_categorical_accuracy: 0.6154\n",
      "Epoch 5/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5756 - categorical_accuracy: 0.7969\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 586us/sample - loss: 1.6096 - categorical_accuracy: 0.7929 - val_loss: 1.8524 - val_categorical_accuracy: 0.6484\n",
      "Epoch 6/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6983 - categorical_accuracy: 0.7812\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 740us/sample - loss: 1.6067 - categorical_accuracy: 0.7870 - val_loss: 1.8483 - val_categorical_accuracy: 0.6484\n",
      "Epoch 7/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.9146 - categorical_accuracy: 0.7458\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 364us/sample - loss: 1.6367 - categorical_accuracy: 0.7663 - val_loss: 1.8577 - val_categorical_accuracy: 0.6154\n",
      "Epoch 8/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6119 - categorical_accuracy: 0.8160\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.5909 - categorical_accuracy: 0.8077 - val_loss: 1.9187 - val_categorical_accuracy: 0.6264\n",
      "Epoch 9/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.6159 - categorical_accuracy: 0.7937\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.6051 - categorical_accuracy: 0.7870 - val_loss: 1.8601 - val_categorical_accuracy: 0.6374\n",
      "Epoch 10/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.6106 - categorical_accuracy: 0.8006\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 210us/sample - loss: 1.6017 - categorical_accuracy: 0.8018 - val_loss: 1.8701 - val_categorical_accuracy: 0.6264\n",
      "Epoch 11/300\n",
      " 96/338 [=======>......................] - ETA: 0s - loss: 1.3006 - categorical_accuracy: 0.7917\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 311us/sample - loss: 1.6246 - categorical_accuracy: 0.7663 - val_loss: 1.8145 - val_categorical_accuracy: 0.6484\n",
      "Epoch 12/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.6350 - categorical_accuracy: 0.8219\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 278us/sample - loss: 1.5908 - categorical_accuracy: 0.8077 - val_loss: 1.8433 - val_categorical_accuracy: 0.6374\n",
      "Epoch 13/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.5756 - categorical_accuracy: 0.8269\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 320us/sample - loss: 1.5907 - categorical_accuracy: 0.7899 - val_loss: 1.8448 - val_categorical_accuracy: 0.6374\n",
      "Epoch 14/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.5197 - categorical_accuracy: 0.8164\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 696us/sample - loss: 1.5885 - categorical_accuracy: 0.8047 - val_loss: 1.8960 - val_categorical_accuracy: 0.6593\n",
      "Epoch 15/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5819 - categorical_accuracy: 0.7917\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 346us/sample - loss: 1.5746 - categorical_accuracy: 0.7929 - val_loss: 1.8288 - val_categorical_accuracy: 0.6593\n",
      "Epoch 16/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5016 - categorical_accuracy: 0.7917\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.5777 - categorical_accuracy: 0.7929 - val_loss: 1.8323 - val_categorical_accuracy: 0.6044\n",
      "Epoch 17/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5423 - categorical_accuracy: 0.8156\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 216us/sample - loss: 1.5730 - categorical_accuracy: 0.8136 - val_loss: 1.8545 - val_categorical_accuracy: 0.6154\n",
      "Epoch 18/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.3422 - categorical_accuracy: 0.8229\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.5896 - categorical_accuracy: 0.8077 - val_loss: 1.8784 - val_categorical_accuracy: 0.6484\n",
      "Epoch 19/300\n",
      "160/338 [=============>................] - ETA: 0s - loss: 1.2350 - categorical_accuracy: 0.8188\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 867us/sample - loss: 1.5800 - categorical_accuracy: 0.8047 - val_loss: 1.8753 - val_categorical_accuracy: 0.6374\n",
      "Epoch 20/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4392 - categorical_accuracy: 0.8346\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 844us/sample - loss: 1.5765 - categorical_accuracy: 0.8136 - val_loss: 1.8117 - val_categorical_accuracy: 0.6374\n",
      "Epoch 21/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5802 - categorical_accuracy: 0.8125\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 358us/sample - loss: 1.5738 - categorical_accuracy: 0.8136 - val_loss: 1.8661 - val_categorical_accuracy: 0.6264\n",
      "Epoch 22/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.6398 - categorical_accuracy: 0.7906\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 409us/sample - loss: 1.5805 - categorical_accuracy: 0.7959 - val_loss: 1.8278 - val_categorical_accuracy: 0.6484\n",
      "Epoch 23/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.6616 - categorical_accuracy: 0.8235\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 252us/sample - loss: 1.5732 - categorical_accuracy: 0.8136 - val_loss: 1.8801 - val_categorical_accuracy: 0.6484\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5693 - categorical_accuracy: 0.8056\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.65934\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.5776 - categorical_accuracy: 0.8077 - val_loss: 1.8241 - val_categorical_accuracy: 0.6484\n",
      "Epoch 25/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5769 - categorical_accuracy: 0.8006\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.65934 to 0.68132, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 0s 284us/sample - loss: 1.5716 - categorical_accuracy: 0.7988 - val_loss: 1.8151 - val_categorical_accuracy: 0.6813\n",
      "Epoch 26/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6095 - categorical_accuracy: 0.8125\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 266us/sample - loss: 1.5701 - categorical_accuracy: 0.8195 - val_loss: 1.8195 - val_categorical_accuracy: 0.6593\n",
      "Epoch 27/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5234 - categorical_accuracy: 0.8125\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 302us/sample - loss: 1.5569 - categorical_accuracy: 0.8107 - val_loss: 1.9036 - val_categorical_accuracy: 0.6264\n",
      "Epoch 28/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.4555 - categorical_accuracy: 0.7885\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 530us/sample - loss: 1.5584 - categorical_accuracy: 0.8195 - val_loss: 1.8852 - val_categorical_accuracy: 0.5714\n",
      "Epoch 29/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.5836 - categorical_accuracy: 0.7978\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 500us/sample - loss: 1.6023 - categorical_accuracy: 0.7781 - val_loss: 1.8248 - val_categorical_accuracy: 0.6484\n",
      "Epoch 30/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4893 - categorical_accuracy: 0.75 - ETA: 0s - loss: 1.4420 - categorical_accuracy: 0.8056\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 417us/sample - loss: 1.5717 - categorical_accuracy: 0.7870 - val_loss: 1.8380 - val_categorical_accuracy: 0.6374\n",
      "Epoch 31/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3873 - categorical_accuracy: 0.8125\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.5623 - categorical_accuracy: 0.8136 - val_loss: 1.8225 - val_categorical_accuracy: 0.6374\n",
      "Epoch 32/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5048 - categorical_accuracy: 0.7857\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 299us/sample - loss: 1.5630 - categorical_accuracy: 0.7929 - val_loss: 1.8263 - val_categorical_accuracy: 0.6374\n",
      "Epoch 33/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4093 - categorical_accuracy: 0.8160\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 240us/sample - loss: 1.5623 - categorical_accuracy: 0.7988 - val_loss: 1.9027 - val_categorical_accuracy: 0.6264\n",
      "Epoch 34/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5722 - categorical_accuracy: 0.8006\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 358us/sample - loss: 1.5659 - categorical_accuracy: 0.8018 - val_loss: 1.8328 - val_categorical_accuracy: 0.6264\n",
      "Epoch 35/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5627 - categorical_accuracy: 0.8036\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 471us/sample - loss: 1.5555 - categorical_accuracy: 0.8047 - val_loss: 1.8256 - val_categorical_accuracy: 0.6374\n",
      "Epoch 36/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5152 - categorical_accuracy: 0.8322\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 400us/sample - loss: 1.5610 - categorical_accuracy: 0.8136 - val_loss: 1.8324 - val_categorical_accuracy: 0.6264\n",
      "Epoch 37/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5126 - categorical_accuracy: 0.8244\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 1ms/sample - loss: 1.5523 - categorical_accuracy: 0.8225 - val_loss: 1.8605 - val_categorical_accuracy: 0.6154\n",
      "Epoch 38/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.6221 - categorical_accuracy: 0.8125\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 391us/sample - loss: 1.5544 - categorical_accuracy: 0.8195 - val_loss: 1.8900 - val_categorical_accuracy: 0.6264\n",
      "Epoch 39/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4142 - categorical_accuracy: 0.8272\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 260us/sample - loss: 1.5537 - categorical_accuracy: 0.8107 - val_loss: 1.8095 - val_categorical_accuracy: 0.6374\n",
      "Epoch 40/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.6369 - categorical_accuracy: 0.8026\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.5522 - categorical_accuracy: 0.8166 - val_loss: 1.8072 - val_categorical_accuracy: 0.6374\n",
      "Epoch 41/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5759 - categorical_accuracy: 0.8125\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.5461 - categorical_accuracy: 0.8136 - val_loss: 1.8011 - val_categorical_accuracy: 0.6593\n",
      "Epoch 42/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.5582 - categorical_accuracy: 0.8466\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 343us/sample - loss: 1.5454 - categorical_accuracy: 0.8107 - val_loss: 1.8065 - val_categorical_accuracy: 0.6484\n",
      "Epoch 43/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 1.4379 - categorical_accuracy: 0.8750\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 311us/sample - loss: 1.5425 - categorical_accuracy: 0.8284 - val_loss: 1.8018 - val_categorical_accuracy: 0.6374\n",
      "Epoch 44/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.7926 - categorical_accuracy: 0.7679\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 589us/sample - loss: 1.5532 - categorical_accuracy: 0.8018 - val_loss: 1.8453 - val_categorical_accuracy: 0.6154\n",
      "Epoch 45/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.5540 - categorical_accuracy: 0.8047\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 565us/sample - loss: 1.5486 - categorical_accuracy: 0.8195 - val_loss: 1.8033 - val_categorical_accuracy: 0.6593\n",
      "Epoch 46/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.6726 - categorical_accuracy: 0.8208\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 272us/sample - loss: 1.5406 - categorical_accuracy: 0.8225 - val_loss: 1.8259 - val_categorical_accuracy: 0.6264\n",
      "Epoch 47/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.4262 - categorical_accuracy: 0.8125\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 376us/sample - loss: 1.5529 - categorical_accuracy: 0.8018 - val_loss: 1.8728 - val_categorical_accuracy: 0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4187 - categorical_accuracy: 0.8191\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 231us/sample - loss: 1.5476 - categorical_accuracy: 0.8107 - val_loss: 1.8596 - val_categorical_accuracy: 0.5934\n",
      "Epoch 49/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.6050 - categorical_accuracy: 0.8191\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 231us/sample - loss: 1.5770 - categorical_accuracy: 0.8225 - val_loss: 1.8161 - val_categorical_accuracy: 0.6484\n",
      "Epoch 50/300\n",
      "160/338 [=============>................] - ETA: 0s - loss: 1.8054 - categorical_accuracy: 0.8062\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 305us/sample - loss: 1.5351 - categorical_accuracy: 0.8254 - val_loss: 1.8085 - val_categorical_accuracy: 0.6374\n",
      "Epoch 51/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.4170 - categorical_accuracy: 0.9375\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 192us/sample - loss: 1.5393 - categorical_accuracy: 0.8107 - val_loss: 1.8142 - val_categorical_accuracy: 0.6154\n",
      "Epoch 52/300\n",
      "160/338 [=============>................] - ETA: 0s - loss: 1.6832 - categorical_accuracy: 0.8375\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 358us/sample - loss: 1.5263 - categorical_accuracy: 0.8343 - val_loss: 1.9034 - val_categorical_accuracy: 0.6813\n",
      "Epoch 53/300\n",
      "160/338 [=============>................] - ETA: 0s - loss: 1.8358 - categorical_accuracy: 0.7688\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 639us/sample - loss: 1.5496 - categorical_accuracy: 0.8018 - val_loss: 1.8346 - val_categorical_accuracy: 0.6374\n",
      "Epoch 54/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.5603 - categorical_accuracy: 0.8021\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 272us/sample - loss: 1.5349 - categorical_accuracy: 0.7988 - val_loss: 1.7938 - val_categorical_accuracy: 0.6703\n",
      "Epoch 55/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5841 - categorical_accuracy: 0.8188\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 406us/sample - loss: 1.5257 - categorical_accuracy: 0.8195 - val_loss: 1.7955 - val_categorical_accuracy: 0.6703\n",
      "Epoch 56/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5546 - categorical_accuracy: 0.8333\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 240us/sample - loss: 1.5233 - categorical_accuracy: 0.8373 - val_loss: 1.7901 - val_categorical_accuracy: 0.6703\n",
      "Epoch 57/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5713 - categorical_accuracy: 0.8021\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.5340 - categorical_accuracy: 0.8107 - val_loss: 1.7997 - val_categorical_accuracy: 0.6484\n",
      "Epoch 58/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6058 - categorical_accuracy: 0.8021\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 278us/sample - loss: 1.5241 - categorical_accuracy: 0.8107 - val_loss: 1.8291 - val_categorical_accuracy: 0.6593\n",
      "Epoch 59/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4852 - categorical_accuracy: 0.8333\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 201us/sample - loss: 1.5257 - categorical_accuracy: 0.8314 - val_loss: 1.7917 - val_categorical_accuracy: 0.6703\n",
      "Epoch 60/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.5280 - categorical_accuracy: 0.8462\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.68132\n",
      "338/338 [==============================] - 0s 329us/sample - loss: 1.5138 - categorical_accuracy: 0.8314 - val_loss: 1.8177 - val_categorical_accuracy: 0.6154\n",
      "Epoch 61/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5830 - categorical_accuracy: 0.8191\n",
      "Epoch 00061: val_categorical_accuracy improved from 0.68132 to 0.70330, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 0s 672us/sample - loss: 1.5245 - categorical_accuracy: 0.8195 - val_loss: 1.7748 - val_categorical_accuracy: 0.7033\n",
      "Epoch 62/300\n",
      "128/338 [==========>...................] - ETA: 0s - loss: 1.5175 - categorical_accuracy: 0.8359\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 364us/sample - loss: 1.5277 - categorical_accuracy: 0.8195 - val_loss: 1.8067 - val_categorical_accuracy: 0.6484\n",
      "Epoch 63/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.5160 - categorical_accuracy: 0.8167\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 379us/sample - loss: 1.5247 - categorical_accuracy: 0.8195 - val_loss: 1.8110 - val_categorical_accuracy: 0.6264\n",
      "Epoch 64/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4348 - categorical_accuracy: 0.8257\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.5115 - categorical_accuracy: 0.8254 - val_loss: 1.8445 - val_categorical_accuracy: 0.6154\n",
      "Epoch 65/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4788 - categorical_accuracy: 0.8194\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.5154 - categorical_accuracy: 0.8166 - val_loss: 1.8034 - val_categorical_accuracy: 0.6374\n",
      "Epoch 66/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5418 - categorical_accuracy: 0.8322\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.5241 - categorical_accuracy: 0.8343 - val_loss: 1.8129 - val_categorical_accuracy: 0.6374\n",
      "Epoch 67/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6705 - categorical_accuracy: 0.8056\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 278us/sample - loss: 1.5148 - categorical_accuracy: 0.8254 - val_loss: 1.7842 - val_categorical_accuracy: 0.6813\n",
      "Epoch 68/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.4461 - categorical_accuracy: 0.8214\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 337us/sample - loss: 1.5126 - categorical_accuracy: 0.8254 - val_loss: 1.8033 - val_categorical_accuracy: 0.6374\n",
      "Epoch 69/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3847 - categorical_accuracy: 0.8281\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 583us/sample - loss: 1.5178 - categorical_accuracy: 0.8225 - val_loss: 1.7871 - val_categorical_accuracy: 0.6593\n",
      "Epoch 70/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4720 - categorical_accuracy: 0.8333\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 456us/sample - loss: 1.5188 - categorical_accuracy: 0.8225 - val_loss: 1.7842 - val_categorical_accuracy: 0.6703\n",
      "Epoch 71/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.2797 - categorical_accuracy: 0.8667\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 373us/sample - loss: 1.5000 - categorical_accuracy: 0.8373 - val_loss: 1.9464 - val_categorical_accuracy: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.6170 - categorical_accuracy: 0.8051\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.5610 - categorical_accuracy: 0.8166 - val_loss: 1.8035 - val_categorical_accuracy: 0.6374\n",
      "Epoch 73/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5865 - categorical_accuracy: 0.8000\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.5223 - categorical_accuracy: 0.8047 - val_loss: 1.7938 - val_categorical_accuracy: 0.6593\n",
      "Epoch 74/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4834 - categorical_accuracy: 0.8191\n",
      "Epoch 00074: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 228us/sample - loss: 1.5086 - categorical_accuracy: 0.8225 - val_loss: 1.7995 - val_categorical_accuracy: 0.6374\n",
      "Epoch 75/300\n",
      " 80/338 [======>.......................] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.8250\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 317us/sample - loss: 1.5094 - categorical_accuracy: 0.8373 - val_loss: 1.8006 - val_categorical_accuracy: 0.6374\n",
      "Epoch 76/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4198 - categorical_accuracy: 0.8272\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 302us/sample - loss: 1.5049 - categorical_accuracy: 0.8195 - val_loss: 1.7783 - val_categorical_accuracy: 0.6813\n",
      "Epoch 77/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.2703 - categorical_accuracy: 0.8646\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 568us/sample - loss: 1.5186 - categorical_accuracy: 0.8284 - val_loss: 1.7887 - val_categorical_accuracy: 0.6813\n",
      "Epoch 78/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.6989 - categorical_accuracy: 0.8250\n",
      "Epoch 00078: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 613us/sample - loss: 1.5099 - categorical_accuracy: 0.8314 - val_loss: 1.8046 - val_categorical_accuracy: 0.6374\n",
      "Epoch 79/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5695 - categorical_accuracy: 0.8125\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.5035 - categorical_accuracy: 0.8195 - val_loss: 1.7991 - val_categorical_accuracy: 0.6374\n",
      "Epoch 80/300\n",
      "144/338 [===========>..................] - ETA: 0s - loss: 1.0278 - categorical_accuracy: 0.8819\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 370us/sample - loss: 1.4976 - categorical_accuracy: 0.8402 - val_loss: 1.7854 - val_categorical_accuracy: 0.6923\n",
      "Epoch 81/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4543 - categorical_accuracy: 0.8299\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 228us/sample - loss: 1.5243 - categorical_accuracy: 0.8314 - val_loss: 1.7916 - val_categorical_accuracy: 0.6593\n",
      "Epoch 82/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4574 - categorical_accuracy: 0.8355\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 219us/sample - loss: 1.5022 - categorical_accuracy: 0.8254 - val_loss: 1.7998 - val_categorical_accuracy: 0.6593\n",
      "Epoch 83/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.4972 - categorical_accuracy: 0.8281\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 305us/sample - loss: 1.5000 - categorical_accuracy: 0.8343 - val_loss: 1.7994 - val_categorical_accuracy: 0.6374\n",
      "Epoch 84/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4988 - categorical_accuracy: 0.8304\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 394us/sample - loss: 1.4940 - categorical_accuracy: 0.8314 - val_loss: 1.7716 - val_categorical_accuracy: 0.6593\n",
      "Epoch 85/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.3799 - categorical_accuracy: 0.8385\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 346us/sample - loss: 1.5073 - categorical_accuracy: 0.8402 - val_loss: 1.8328 - val_categorical_accuracy: 0.6374\n",
      "Epoch 86/300\n",
      "160/338 [=============>................] - ETA: 0s - loss: 1.3985 - categorical_accuracy: 0.8000\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 867us/sample - loss: 1.5007 - categorical_accuracy: 0.8225 - val_loss: 1.7969 - val_categorical_accuracy: 0.6154\n",
      "Epoch 87/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4903 - categorical_accuracy: 0.8235\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 459us/sample - loss: 1.4910 - categorical_accuracy: 0.8314 - val_loss: 1.7989 - val_categorical_accuracy: 0.6593\n",
      "Epoch 88/300\n",
      " 80/338 [======>.......................] - ETA: 0s - loss: 1.9733 - categorical_accuracy: 0.7500\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 391us/sample - loss: 1.4958 - categorical_accuracy: 0.8195 - val_loss: 1.7867 - val_categorical_accuracy: 0.6593\n",
      "Epoch 89/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4630 - categorical_accuracy: 0.8191\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.4904 - categorical_accuracy: 0.8254 - val_loss: 1.7950 - val_categorical_accuracy: 0.6374\n",
      "Epoch 90/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4891 - categorical_accuracy: 0.8194\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.5070 - categorical_accuracy: 0.8225 - val_loss: 1.8034 - val_categorical_accuracy: 0.6484\n",
      "Epoch 91/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5256 - categorical_accuracy: 0.8036\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 619us/sample - loss: 1.5143 - categorical_accuracy: 0.8077 - val_loss: 1.7898 - val_categorical_accuracy: 0.6593\n",
      "Epoch 92/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4636 - categorical_accuracy: 0.8250\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.4985 - categorical_accuracy: 0.8254 - val_loss: 1.8118 - val_categorical_accuracy: 0.6264\n",
      "Epoch 93/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4590 - categorical_accuracy: 0.8250\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 669us/sample - loss: 1.4956 - categorical_accuracy: 0.8254 - val_loss: 1.7935 - val_categorical_accuracy: 0.6264\n",
      "Epoch 94/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5049 - categorical_accuracy: 0.8344\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 465us/sample - loss: 1.4927 - categorical_accuracy: 0.8284 - val_loss: 1.7737 - val_categorical_accuracy: 0.6484\n",
      "Epoch 95/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4646 - categorical_accuracy: 0.8355\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 388us/sample - loss: 1.4897 - categorical_accuracy: 0.8373 - val_loss: 1.7737 - val_categorical_accuracy: 0.6703\n",
      "Epoch 96/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.2956 - categorical_accuracy: 0.8359\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 263us/sample - loss: 1.4864 - categorical_accuracy: 0.8314 - val_loss: 1.7912 - val_categorical_accuracy: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4671 - categorical_accuracy: 0.8229\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.4931 - categorical_accuracy: 0.8284 - val_loss: 1.7826 - val_categorical_accuracy: 0.6813\n",
      "Epoch 98/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4565 - categorical_accuracy: 0.8388\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.4885 - categorical_accuracy: 0.8343 - val_loss: 1.7690 - val_categorical_accuracy: 0.6593\n",
      "Epoch 99/300\n",
      "128/338 [==========>...................] - ETA: 0s - loss: 1.8152 - categorical_accuracy: 0.7734\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 334us/sample - loss: 1.4957 - categorical_accuracy: 0.8166 - val_loss: 1.7844 - val_categorical_accuracy: 0.6703\n",
      "Epoch 100/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.3482 - categorical_accuracy: 0.8368\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 562us/sample - loss: 1.4873 - categorical_accuracy: 0.8284 - val_loss: 1.7832 - val_categorical_accuracy: 0.6593\n",
      "Epoch 101/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.4997 - categorical_accuracy: 0.8333\n",
      "Epoch 00101: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 539us/sample - loss: 1.4844 - categorical_accuracy: 0.8432 - val_loss: 1.8005 - val_categorical_accuracy: 0.6374\n",
      "Epoch 102/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3505 - categorical_accuracy: 0.8382\n",
      "Epoch 00102: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 779us/sample - loss: 1.4814 - categorical_accuracy: 0.8254 - val_loss: 1.7936 - val_categorical_accuracy: 0.6374\n",
      "Epoch 103/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3927 - categorical_accuracy: 0.8419\n",
      "Epoch 00103: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.4785 - categorical_accuracy: 0.8343 - val_loss: 1.7711 - val_categorical_accuracy: 0.6923\n",
      "Epoch 104/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.5759 - categorical_accuracy: 0.8269\n",
      "Epoch 00104: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 432us/sample - loss: 1.4797 - categorical_accuracy: 0.8284 - val_loss: 1.7717 - val_categorical_accuracy: 0.6703\n",
      "Epoch 105/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4022 - categorical_accuracy: 0.8553\n",
      "Epoch 00105: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.4843 - categorical_accuracy: 0.8462 - val_loss: 1.8082 - val_categorical_accuracy: 0.6593\n",
      "Epoch 106/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5484 - categorical_accuracy: 0.8438\n",
      "Epoch 00106: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.4725 - categorical_accuracy: 0.8432 - val_loss: 1.7783 - val_categorical_accuracy: 0.6593\n",
      "Epoch 107/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4777 - categorical_accuracy: 0.8363\n",
      "Epoch 00107: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 364us/sample - loss: 1.4702 - categorical_accuracy: 0.8373 - val_loss: 1.8023 - val_categorical_accuracy: 0.6374\n",
      "Epoch 108/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 2.3171 - categorical_accuracy: 0.7500\n",
      "Epoch 00108: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 175us/sample - loss: 1.4761 - categorical_accuracy: 0.8314 - val_loss: 1.7837 - val_categorical_accuracy: 0.6374\n",
      "Epoch 109/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4877 - categorical_accuracy: 0.8313\n",
      "Epoch 00109: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 426us/sample - loss: 1.4739 - categorical_accuracy: 0.8314 - val_loss: 1.7996 - val_categorical_accuracy: 0.6154\n",
      "Epoch 110/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5326 - categorical_accuracy: 0.8454\n",
      "Epoch 00110: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 802us/sample - loss: 1.4671 - categorical_accuracy: 0.8402 - val_loss: 1.7736 - val_categorical_accuracy: 0.6593\n",
      "Epoch 111/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2783 - categorical_accuracy: 0.8611\n",
      "Epoch 00111: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 293us/sample - loss: 1.4695 - categorical_accuracy: 0.8432 - val_loss: 1.8348 - val_categorical_accuracy: 0.6374\n",
      "Epoch 112/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4057 - categorical_accuracy: 0.8272\n",
      "Epoch 00112: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 326us/sample - loss: 1.4748 - categorical_accuracy: 0.8284 - val_loss: 1.7818 - val_categorical_accuracy: 0.6593\n",
      "Epoch 113/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6026 - categorical_accuracy: 0.8368\n",
      "Epoch 00113: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.4708 - categorical_accuracy: 0.8402 - val_loss: 1.7662 - val_categorical_accuracy: 0.6593\n",
      "Epoch 114/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4379 - categorical_accuracy: 0.8618\n",
      "Epoch 00114: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.4749 - categorical_accuracy: 0.8521 - val_loss: 1.8115 - val_categorical_accuracy: 0.6813\n",
      "Epoch 115/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4858 - categorical_accuracy: 0.8313\n",
      "Epoch 00115: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 367us/sample - loss: 1.4724 - categorical_accuracy: 0.8343 - val_loss: 1.7785 - val_categorical_accuracy: 0.6593\n",
      "Epoch 116/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4793 - categorical_accuracy: 0.8512\n",
      "Epoch 00116: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 293us/sample - loss: 1.4717 - categorical_accuracy: 0.8521 - val_loss: 1.7823 - val_categorical_accuracy: 0.6593\n",
      "Epoch 117/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.3078 - categorical_accuracy: 0.8616\n",
      "Epoch 00117: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 314us/sample - loss: 1.4672 - categorical_accuracy: 0.8314 - val_loss: 1.7812 - val_categorical_accuracy: 0.6593\n",
      "Epoch 118/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4620 - categorical_accuracy: 0.8403\n",
      "Epoch 00118: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 1ms/sample - loss: 1.4751 - categorical_accuracy: 0.8462 - val_loss: 1.7794 - val_categorical_accuracy: 0.6484\n",
      "Epoch 119/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.3306 - categorical_accuracy: 0.8413\n",
      "Epoch 00119: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 269us/sample - loss: 1.4622 - categorical_accuracy: 0.8373 - val_loss: 1.7657 - val_categorical_accuracy: 0.6813\n",
      "Epoch 120/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4995 - categorical_accuracy: 0.8289\n",
      "Epoch 00120: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 388us/sample - loss: 1.4697 - categorical_accuracy: 0.8373 - val_loss: 1.7875 - val_categorical_accuracy: 0.6484\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4371 - categorical_accuracy: 0.8487\n",
      "Epoch 00121: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 231us/sample - loss: 1.4710 - categorical_accuracy: 0.8432 - val_loss: 1.7651 - val_categorical_accuracy: 0.6703\n",
      "Epoch 122/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4687 - categorical_accuracy: 0.8562\n",
      "Epoch 00122: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 216us/sample - loss: 1.4572 - categorical_accuracy: 0.8550 - val_loss: 1.7635 - val_categorical_accuracy: 0.6703\n",
      "Epoch 123/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4944 - categorical_accuracy: 0.8355\n",
      "Epoch 00123: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 308us/sample - loss: 1.4660 - categorical_accuracy: 0.8432 - val_loss: 1.7891 - val_categorical_accuracy: 0.6703\n",
      "Epoch 124/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 1.3226 - categorical_accuracy: 0.8750\n",
      "Epoch 00124: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 189us/sample - loss: 1.4603 - categorical_accuracy: 0.8491 - val_loss: 1.8156 - val_categorical_accuracy: 0.6484\n",
      "Epoch 125/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.7134 - categorical_accuracy: 0.8047\n",
      "Epoch 00125: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 329us/sample - loss: 1.4683 - categorical_accuracy: 0.8284 - val_loss: 1.7899 - val_categorical_accuracy: 0.6703\n",
      "Epoch 126/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4586 - categorical_accuracy: 0.8456\n",
      "Epoch 00126: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 927us/sample - loss: 1.4745 - categorical_accuracy: 0.8402 - val_loss: 1.7835 - val_categorical_accuracy: 0.6484\n",
      "Epoch 127/300\n",
      "128/338 [==========>...................] - ETA: 0s - loss: 0.7324 - categorical_accuracy: 0.8594\n",
      "Epoch 00127: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 417us/sample - loss: 1.4580 - categorical_accuracy: 0.8491 - val_loss: 1.7869 - val_categorical_accuracy: 0.6593\n",
      "Epoch 128/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4867 - categorical_accuracy: 0.8388\n",
      "Epoch 00128: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 400us/sample - loss: 1.4707 - categorical_accuracy: 0.8373 - val_loss: 1.8123 - val_categorical_accuracy: 0.6374\n",
      "Epoch 129/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5647 - categorical_accuracy: 0.8090\n",
      "Epoch 00129: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.4650 - categorical_accuracy: 0.8254 - val_loss: 1.7799 - val_categorical_accuracy: 0.6593\n",
      "Epoch 130/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4769 - categorical_accuracy: 0.8355\n",
      "Epoch 00130: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.4553 - categorical_accuracy: 0.8373 - val_loss: 1.8131 - val_categorical_accuracy: 0.6703\n",
      "Epoch 131/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2194 - categorical_accuracy: 0.8542\n",
      "Epoch 00131: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.4641 - categorical_accuracy: 0.8402 - val_loss: 1.7717 - val_categorical_accuracy: 0.6703\n",
      "Epoch 132/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5149 - categorical_accuracy: 0.8469\n",
      "Epoch 00132: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 731us/sample - loss: 1.4573 - categorical_accuracy: 0.8432 - val_loss: 1.7591 - val_categorical_accuracy: 0.6703\n",
      "Epoch 133/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.4597 - categorical_accuracy: 0.8182\n",
      "Epoch 00133: val_categorical_accuracy did not improve from 0.70330\n",
      "338/338 [==============================] - 0s 349us/sample - loss: 1.4630 - categorical_accuracy: 0.8491 - val_loss: 1.7822 - val_categorical_accuracy: 0.6813\n",
      "Epoch 134/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4719 - categorical_accuracy: 0.8388\n",
      "Epoch 00134: val_categorical_accuracy improved from 0.70330 to 0.72527, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 0s 622us/sample - loss: 1.4510 - categorical_accuracy: 0.8402 - val_loss: 1.7517 - val_categorical_accuracy: 0.7253\n",
      "Epoch 135/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.3137 - categorical_accuracy: 0.8398\n",
      "Epoch 00135: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 263us/sample - loss: 1.4501 - categorical_accuracy: 0.8402 - val_loss: 1.7661 - val_categorical_accuracy: 0.6593\n",
      "Epoch 136/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5350 - categorical_accuracy: 0.8393\n",
      "Epoch 00136: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 364us/sample - loss: 1.4570 - categorical_accuracy: 0.8432 - val_loss: 1.8091 - val_categorical_accuracy: 0.6154\n",
      "Epoch 137/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2096 - categorical_accuracy: 0.8611\n",
      "Epoch 00137: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 444us/sample - loss: 1.4519 - categorical_accuracy: 0.8491 - val_loss: 1.7679 - val_categorical_accuracy: 0.7033\n",
      "Epoch 138/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.5441 - categorical_accuracy: 0.8281\n",
      "Epoch 00138: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 281us/sample - loss: 1.4739 - categorical_accuracy: 0.8343 - val_loss: 1.7667 - val_categorical_accuracy: 0.6813\n",
      "Epoch 139/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4681 - categorical_accuracy: 0.8344\n",
      "Epoch 00139: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 645us/sample - loss: 1.4546 - categorical_accuracy: 0.8373 - val_loss: 1.7635 - val_categorical_accuracy: 0.6593\n",
      "Epoch 140/300\n",
      "112/338 [========>.....................] - ETA: 0s - loss: 1.4826 - categorical_accuracy: 0.8482\n",
      "Epoch 00140: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 305us/sample - loss: 1.4565 - categorical_accuracy: 0.8432 - val_loss: 1.7550 - val_categorical_accuracy: 0.6703\n",
      "Epoch 141/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5237 - categorical_accuracy: 0.8651\n",
      "Epoch 00141: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 483us/sample - loss: 1.4532 - categorical_accuracy: 0.8580 - val_loss: 1.8196 - val_categorical_accuracy: 0.6593\n",
      "Epoch 142/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4123 - categorical_accuracy: 0.8631\n",
      "Epoch 00142: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 397us/sample - loss: 1.4533 - categorical_accuracy: 0.8609 - val_loss: 1.7595 - val_categorical_accuracy: 0.6703\n",
      "Epoch 143/300\n",
      " 48/338 [===>..........................] - ETA: 0s - loss: 0.9944 - categorical_accuracy: 0.8750\n",
      "Epoch 00143: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 332us/sample - loss: 1.4506 - categorical_accuracy: 0.8462 - val_loss: 1.7728 - val_categorical_accuracy: 0.6484\n",
      "Epoch 144/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.1305 - categorical_accuracy: 0.8403\n",
      "Epoch 00144: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 305us/sample - loss: 1.4635 - categorical_accuracy: 0.8284 - val_loss: 1.7696 - val_categorical_accuracy: 0.6703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.5996 - categorical_accuracy: 0.86 - ETA: 0s - loss: 1.4432 - categorical_accuracy: 0.8512\n",
      "Epoch 00145: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 503us/sample - loss: 1.4394 - categorical_accuracy: 0.8491 - val_loss: 1.7929 - val_categorical_accuracy: 0.6484\n",
      "Epoch 146/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.4149 - categorical_accuracy: 0.8125\n",
      "Epoch 00146: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 172us/sample - loss: 1.4585 - categorical_accuracy: 0.8462 - val_loss: 1.8208 - val_categorical_accuracy: 0.6374\n",
      "Epoch 147/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.5729 - categorical_accuracy: 0.8073\n",
      "Epoch 00147: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 349us/sample - loss: 1.4616 - categorical_accuracy: 0.8166 - val_loss: 1.7844 - val_categorical_accuracy: 0.6593\n",
      "Epoch 148/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5564 - categorical_accuracy: 0.8299\n",
      "Epoch 00148: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 403us/sample - loss: 1.4652 - categorical_accuracy: 0.8373 - val_loss: 1.7546 - val_categorical_accuracy: 0.6703\n",
      "Epoch 149/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.3297 - categorical_accuracy: 1.0000\n",
      "Epoch 00149: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 175us/sample - loss: 1.4438 - categorical_accuracy: 0.8550 - val_loss: 1.7545 - val_categorical_accuracy: 0.6813\n",
      "Epoch 150/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.2465 - categorical_accuracy: 0.8542\n",
      "Epoch 00150: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 619us/sample - loss: 1.4438 - categorical_accuracy: 0.8402 - val_loss: 1.7588 - val_categorical_accuracy: 0.6923\n",
      "Epoch 151/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3930 - categorical_accuracy: 0.8375\n",
      "Epoch 00151: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 400us/sample - loss: 1.4690 - categorical_accuracy: 0.8343 - val_loss: 1.7533 - val_categorical_accuracy: 0.6813\n",
      "Epoch 152/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 2.2093 - categorical_accuracy: 0.8750\n",
      "Epoch 00152: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 172us/sample - loss: 1.4407 - categorical_accuracy: 0.8432 - val_loss: 1.7662 - val_categorical_accuracy: 0.6813\n",
      "Epoch 153/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4620 - categorical_accuracy: 0.8406\n",
      "Epoch 00153: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 459us/sample - loss: 1.4418 - categorical_accuracy: 0.8462 - val_loss: 1.7642 - val_categorical_accuracy: 0.6813\n",
      "Epoch 154/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6379 - categorical_accuracy: 0.8438\n",
      "Epoch 00154: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 601us/sample - loss: 1.4439 - categorical_accuracy: 0.8550 - val_loss: 1.8223 - val_categorical_accuracy: 0.6154\n",
      "Epoch 155/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.3926 - categorical_accuracy: 0.8750\n",
      "Epoch 00155: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 198us/sample - loss: 1.4582 - categorical_accuracy: 0.8314 - val_loss: 1.7815 - val_categorical_accuracy: 0.6484\n",
      "Epoch 156/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.2770 - categorical_accuracy: 0.8636\n",
      "Epoch 00156: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 343us/sample - loss: 1.4405 - categorical_accuracy: 0.8432 - val_loss: 1.7696 - val_categorical_accuracy: 0.6813\n",
      "Epoch 157/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.7247 - categorical_accuracy: 0.8333\n",
      "Epoch 00157: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 320us/sample - loss: 1.4460 - categorical_accuracy: 0.8432 - val_loss: 1.7410 - val_categorical_accuracy: 0.6923\n",
      "Epoch 158/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 1.2556 - categorical_accuracy: 0.9375\n",
      "Epoch 00158: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 175us/sample - loss: 1.4485 - categorical_accuracy: 0.8373 - val_loss: 1.7594 - val_categorical_accuracy: 0.6593\n",
      "Epoch 159/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.6267 - categorical_accuracy: 0.8636\n",
      "Epoch 00159: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 400us/sample - loss: 1.4326 - categorical_accuracy: 0.8580 - val_loss: 1.7517 - val_categorical_accuracy: 0.6813\n",
      "Epoch 160/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.6084 - categorical_accuracy: 0.8229\n",
      "Epoch 00160: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 332us/sample - loss: 1.4594 - categorical_accuracy: 0.8491 - val_loss: 1.7651 - val_categorical_accuracy: 0.6593\n",
      "Epoch 161/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.2407 - categorical_accuracy: 1.0000\n",
      "Epoch 00161: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 186us/sample - loss: 1.4364 - categorical_accuracy: 0.8491 - val_loss: 1.7775 - val_categorical_accuracy: 0.6703\n",
      "Epoch 162/300\n",
      "144/338 [===========>..................] - ETA: 0s - loss: 1.3977 - categorical_accuracy: 0.8611\n",
      "Epoch 00162: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 337us/sample - loss: 1.4336 - categorical_accuracy: 0.8521 - val_loss: 1.7632 - val_categorical_accuracy: 0.6593\n",
      "Epoch 163/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.6760 - categorical_accuracy: 0.8375\n",
      "Epoch 00163: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 349us/sample - loss: 1.4386 - categorical_accuracy: 0.8521 - val_loss: 1.7892 - val_categorical_accuracy: 0.6703\n",
      "Epoch 164/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3496 - categorical_accuracy: 0.8625\n",
      "Epoch 00164: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 329us/sample - loss: 1.4366 - categorical_accuracy: 0.8550 - val_loss: 1.7756 - val_categorical_accuracy: 0.6703\n",
      "Epoch 165/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.1558 - categorical_accuracy: 0.8510\n",
      "Epoch 00165: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 521us/sample - loss: 1.4352 - categorical_accuracy: 0.8402 - val_loss: 1.8194 - val_categorical_accuracy: 0.6484\n",
      "Epoch 166/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.2271 - categorical_accuracy: 0.8242 ETA: 0s - loss: 1.1994 - categorical_accuracy: 0.81\n",
      "Epoch 00166: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 391us/sample - loss: 1.4589 - categorical_accuracy: 0.8314 - val_loss: 1.7796 - val_categorical_accuracy: 0.6703\n",
      "Epoch 167/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 0.3081 - categorical_accuracy: 0.8750\n",
      "Epoch 00167: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 198us/sample - loss: 1.4362 - categorical_accuracy: 0.8550 - val_loss: 1.7563 - val_categorical_accuracy: 0.6923\n",
      "Epoch 168/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.6476 - categorical_accuracy: 0.8333\n",
      "Epoch 00168: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 633us/sample - loss: 1.4446 - categorical_accuracy: 0.8550 - val_loss: 1.7561 - val_categorical_accuracy: 0.6703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.2165 - categorical_accuracy: 0.8636\n",
      "Epoch 00169: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 337us/sample - loss: 1.4522 - categorical_accuracy: 0.8373 - val_loss: 1.7679 - val_categorical_accuracy: 0.6923\n",
      "Epoch 170/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4544 - categorical_accuracy: 0.8469\n",
      "Epoch 00170: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 346us/sample - loss: 1.4373 - categorical_accuracy: 0.8462 - val_loss: 1.7775 - val_categorical_accuracy: 0.6484\n",
      "Epoch 171/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.8252 - categorical_accuracy: 0.8170\n",
      "Epoch 00171: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 352us/sample - loss: 1.4596 - categorical_accuracy: 0.8402 - val_loss: 1.7928 - val_categorical_accuracy: 0.6374\n",
      "Epoch 172/300\n",
      " 64/338 [====>.........................] - ETA: 0s - loss: 1.0864 - categorical_accuracy: 0.8594\n",
      "Epoch 00172: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 326us/sample - loss: 1.4345 - categorical_accuracy: 0.8550 - val_loss: 1.7774 - val_categorical_accuracy: 0.6484\n",
      "Epoch 173/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.6449 - categorical_accuracy: 0.82 - ETA: 0s - loss: 1.4668 - categorical_accuracy: 0.8368\n",
      "Epoch 00173: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 568us/sample - loss: 1.4359 - categorical_accuracy: 0.8432 - val_loss: 1.7683 - val_categorical_accuracy: 0.6813\n",
      "Epoch 174/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5185 - categorical_accuracy: 0.8393\n",
      "Epoch 00174: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 349us/sample - loss: 1.4327 - categorical_accuracy: 0.8521 - val_loss: 1.7551 - val_categorical_accuracy: 0.7143\n",
      "Epoch 175/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 4.1840 - categorical_accuracy: 0.7500\n",
      "Epoch 00175: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 317us/sample - loss: 1.4276 - categorical_accuracy: 0.8698 - val_loss: 1.8183 - val_categorical_accuracy: 0.6593\n",
      "Epoch 176/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5098 - categorical_accuracy: 0.8520\n",
      "Epoch 00176: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 388us/sample - loss: 1.4413 - categorical_accuracy: 0.8491 - val_loss: 1.8173 - val_categorical_accuracy: 0.6484\n",
      "Epoch 177/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.3480 - categorical_accuracy: 0.8438\n",
      "Epoch 00177: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 512us/sample - loss: 1.4862 - categorical_accuracy: 0.8284 - val_loss: 1.7775 - val_categorical_accuracy: 0.6593\n",
      "Epoch 178/300\n",
      " 96/338 [=======>......................] - ETA: 0s - loss: 0.8104 - categorical_accuracy: 0.8542\n",
      "Epoch 00178: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 311us/sample - loss: 1.4475 - categorical_accuracy: 0.8521 - val_loss: 1.7752 - val_categorical_accuracy: 0.6703\n",
      "Epoch 179/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4345 - categorical_accuracy: 0.8423\n",
      "Epoch 00179: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 258us/sample - loss: 1.4274 - categorical_accuracy: 0.8432 - val_loss: 1.7805 - val_categorical_accuracy: 0.6593\n",
      "Epoch 180/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5254 - categorical_accuracy: 0.8482\n",
      "Epoch 00180: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 361us/sample - loss: 1.4312 - categorical_accuracy: 0.8580 - val_loss: 1.7701 - val_categorical_accuracy: 0.6484\n",
      "Epoch 181/300\n",
      " 80/338 [======>.......................] - ETA: 0s - loss: 0.7307 - categorical_accuracy: 0.8250\n",
      "Epoch 00181: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 335us/sample - loss: 1.4504 - categorical_accuracy: 0.8225 - val_loss: 1.7465 - val_categorical_accuracy: 0.6593\n",
      "Epoch 182/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4252 - categorical_accuracy: 0.8601\n",
      "Epoch 00182: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.4204 - categorical_accuracy: 0.8609 - val_loss: 1.8708 - val_categorical_accuracy: 0.6593\n",
      "Epoch 183/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.4404 - categorical_accuracy: 0.8438\n",
      "Epoch 00183: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 355us/sample - loss: 1.4521 - categorical_accuracy: 0.8373 - val_loss: 1.7410 - val_categorical_accuracy: 0.7033\n",
      "Epoch 184/300\n",
      " 48/338 [===>..........................] - ETA: 0s - loss: 0.9759 - categorical_accuracy: 0.8958\n",
      "Epoch 00184: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 329us/sample - loss: 1.4272 - categorical_accuracy: 0.8432 - val_loss: 1.7519 - val_categorical_accuracy: 0.7143\n",
      "Epoch 185/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.3411 - categorical_accuracy: 0.8542\n",
      "Epoch 00185: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 308us/sample - loss: 1.4206 - categorical_accuracy: 0.8491 - val_loss: 1.7669 - val_categorical_accuracy: 0.6703\n",
      "Epoch 186/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.7520 - categorical_accuracy: 0.8295\n",
      "Epoch 00186: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 471us/sample - loss: 1.4211 - categorical_accuracy: 0.8728 - val_loss: 1.7699 - val_categorical_accuracy: 0.6374\n",
      "Epoch 187/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3456 - categorical_accuracy: 0.8531\n",
      "Epoch 00187: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 213us/sample - loss: 1.4288 - categorical_accuracy: 0.8462 - val_loss: 1.7462 - val_categorical_accuracy: 0.6813\n",
      "Epoch 188/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.5080 - categorical_accuracy: 0.8322\n",
      "Epoch 00188: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 417us/sample - loss: 1.4324 - categorical_accuracy: 0.8373 - val_loss: 1.7561 - val_categorical_accuracy: 0.7033\n",
      "Epoch 189/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.1547 - categorical_accuracy: 0.8789\n",
      "Epoch 00189: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 480us/sample - loss: 1.4196 - categorical_accuracy: 0.8609 - val_loss: 1.7742 - val_categorical_accuracy: 0.6923\n",
      "Epoch 190/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4299 - categorical_accuracy: 0.8542\n",
      "Epoch 00190: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 376us/sample - loss: 1.4226 - categorical_accuracy: 0.8550 - val_loss: 1.7746 - val_categorical_accuracy: 0.6703\n",
      "Epoch 191/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.3340 - categorical_accuracy: 0.8750\n",
      "Epoch 00191: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 506us/sample - loss: 1.4311 - categorical_accuracy: 0.8550 - val_loss: 1.7621 - val_categorical_accuracy: 0.6923\n",
      "Epoch 192/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4474 - categorical_accuracy: 0.8553\n",
      "Epoch 00192: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 764us/sample - loss: 1.4365 - categorical_accuracy: 0.8462 - val_loss: 1.7645 - val_categorical_accuracy: 0.6813\n",
      "Epoch 193/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4054 - categorical_accuracy: 0.8576\n",
      "Epoch 00193: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 263us/sample - loss: 1.4273 - categorical_accuracy: 0.8580 - val_loss: 1.7574 - val_categorical_accuracy: 0.6703\n",
      "Epoch 194/300\n",
      "144/338 [===========>..................] - ETA: 0s - loss: 1.2834 - categorical_accuracy: 0.8819\n",
      "Epoch 00194: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 411us/sample - loss: 1.4098 - categorical_accuracy: 0.8609 - val_loss: 1.7640 - val_categorical_accuracy: 0.6703\n",
      "Epoch 195/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4817 - categorical_accuracy: 0.8553\n",
      "Epoch 00195: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.4171 - categorical_accuracy: 0.8521 - val_loss: 1.8276 - val_categorical_accuracy: 0.6593\n",
      "Epoch 196/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4909 - categorical_accuracy: 0.8469\n",
      "Epoch 00196: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 210us/sample - loss: 1.4279 - categorical_accuracy: 0.8521 - val_loss: 1.7366 - val_categorical_accuracy: 0.6923\n",
      "Epoch 197/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.4834 - categorical_accuracy: 0.8542\n",
      "Epoch 00197: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 343us/sample - loss: 1.4104 - categorical_accuracy: 0.8639 - val_loss: 1.8461 - val_categorical_accuracy: 0.6484\n",
      "Epoch 198/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4303 - categorical_accuracy: 0.8452\n",
      "Epoch 00198: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 275us/sample - loss: 1.4262 - categorical_accuracy: 0.8432 - val_loss: 1.7516 - val_categorical_accuracy: 0.6813\n",
      "Epoch 199/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.5115 - categorical_accuracy: 0.8385\n",
      "Epoch 00199: val_categorical_accuracy did not improve from 0.72527\n",
      "338/338 [==============================] - 0s 432us/sample - loss: 1.4203 - categorical_accuracy: 0.8550 - val_loss: 1.8185 - val_categorical_accuracy: 0.6593\n",
      "Epoch 200/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4535 - categorical_accuracy: 0.8520\n",
      "Epoch 00200: val_categorical_accuracy improved from 0.72527 to 0.73626, saving model to ..\\..\\Trained Models\\Transfered_Models\\Fully_Connected_Layers_Only\\Pavia\\Fully_Connected_Layers_for_Pavia_With_source_data_as_Pavia44_overlap_ratio_50_percent.h5\n",
      "338/338 [==============================] - 0s 968us/sample - loss: 1.4234 - categorical_accuracy: 0.8580 - val_loss: 1.7432 - val_categorical_accuracy: 0.7363\n",
      "Epoch 201/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3389 - categorical_accuracy: 0.8684\n",
      "Epoch 00201: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 364us/sample - loss: 1.4159 - categorical_accuracy: 0.8669 - val_loss: 1.7348 - val_categorical_accuracy: 0.6923\n",
      "Epoch 202/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4742 - categorical_accuracy: 0.8656\n",
      "Epoch 00202: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 219us/sample - loss: 1.4156 - categorical_accuracy: 0.8639 - val_loss: 1.7538 - val_categorical_accuracy: 0.6703\n",
      "Epoch 203/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3801 - categorical_accuracy: 0.8684\n",
      "Epoch 00203: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 219us/sample - loss: 1.4126 - categorical_accuracy: 0.8669 - val_loss: 1.7668 - val_categorical_accuracy: 0.6593\n",
      "Epoch 204/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.1709 - categorical_accuracy: 0.8684\n",
      "Epoch 00204: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.4116 - categorical_accuracy: 0.8609 - val_loss: 1.7367 - val_categorical_accuracy: 0.6813\n",
      "Epoch 205/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4509 - categorical_accuracy: 0.8542\n",
      "Epoch 00205: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 320us/sample - loss: 1.4251 - categorical_accuracy: 0.8521 - val_loss: 1.7622 - val_categorical_accuracy: 0.6813\n",
      "Epoch 206/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 1.3303 - categorical_accuracy: 0.8125\n",
      "Epoch 00206: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 184us/sample - loss: 1.4156 - categorical_accuracy: 0.8550 - val_loss: 1.7610 - val_categorical_accuracy: 0.6703\n",
      "Epoch 207/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3751 - categorical_accuracy: 0.8625\n",
      "Epoch 00207: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 684us/sample - loss: 1.4083 - categorical_accuracy: 0.8639 - val_loss: 1.7896 - val_categorical_accuracy: 0.6484\n",
      "Epoch 208/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.2672 - categorical_accuracy: 0.8719\n",
      "Epoch 00208: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 687us/sample - loss: 1.4097 - categorical_accuracy: 0.8580 - val_loss: 1.7445 - val_categorical_accuracy: 0.6923\n",
      "Epoch 209/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.6259 - categorical_accuracy: 0.8583\n",
      "Epoch 00209: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 687us/sample - loss: 1.4077 - categorical_accuracy: 0.8728 - val_loss: 1.7643 - val_categorical_accuracy: 0.6703\n",
      "Epoch 210/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3300 - categorical_accuracy: 0.8787\n",
      "Epoch 00210: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 548us/sample - loss: 1.4086 - categorical_accuracy: 0.8669 - val_loss: 1.8018 - val_categorical_accuracy: 0.6593\n",
      "Epoch 211/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4384 - categorical_accuracy: 0.8611\n",
      "Epoch 00211: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 266us/sample - loss: 1.4112 - categorical_accuracy: 0.8580 - val_loss: 1.7451 - val_categorical_accuracy: 0.6813\n",
      "Epoch 212/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4214 - categorical_accuracy: 0.8684\n",
      "Epoch 00212: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 228us/sample - loss: 1.4092 - categorical_accuracy: 0.8669 - val_loss: 1.7343 - val_categorical_accuracy: 0.7143\n",
      "Epoch 213/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4825 - categorical_accuracy: 0.8594\n",
      "Epoch 00213: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 269us/sample - loss: 1.4155 - categorical_accuracy: 0.8639 - val_loss: 1.7586 - val_categorical_accuracy: 0.6813\n",
      "Epoch 214/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4099 - categorical_accuracy: 0.8661\n",
      "Epoch 00214: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 210us/sample - loss: 1.4016 - categorical_accuracy: 0.8669 - val_loss: 1.7549 - val_categorical_accuracy: 0.6813\n",
      "Epoch 215/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.3359 - categorical_accuracy: 0.8875\n",
      "Epoch 00215: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 311us/sample - loss: 1.4071 - categorical_accuracy: 0.8639 - val_loss: 1.7646 - val_categorical_accuracy: 0.6813\n",
      "Epoch 216/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4365 - categorical_accuracy: 0.8452\n",
      "Epoch 00216: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 998us/sample - loss: 1.4282 - categorical_accuracy: 0.8462 - val_loss: 1.7479 - val_categorical_accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3742 - categorical_accuracy: 0.8651\n",
      "Epoch 00217: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 684us/sample - loss: 1.4073 - categorical_accuracy: 0.8609 - val_loss: 1.7410 - val_categorical_accuracy: 0.7033\n",
      "Epoch 218/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5337 - categorical_accuracy: 0.8681\n",
      "Epoch 00218: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 376us/sample - loss: 1.4149 - categorical_accuracy: 0.8669 - val_loss: 1.7614 - val_categorical_accuracy: 0.6813\n",
      "Epoch 219/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3782 - categorical_accuracy: 0.8586\n",
      "Epoch 00219: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 231us/sample - loss: 1.4088 - categorical_accuracy: 0.8550 - val_loss: 1.7654 - val_categorical_accuracy: 0.6813\n",
      "Epoch 220/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4726 - categorical_accuracy: 0.8531\n",
      "Epoch 00220: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.4089 - categorical_accuracy: 0.8580 - val_loss: 1.7441 - val_categorical_accuracy: 0.6923\n",
      "Epoch 221/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3101 - categorical_accuracy: 0.8882\n",
      "Epoch 00221: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.4039 - categorical_accuracy: 0.8787 - val_loss: 1.7982 - val_categorical_accuracy: 0.6593\n",
      "Epoch 222/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.3120 - categorical_accuracy: 0.81 - ETA: 0s - loss: 1.4023 - categorical_accuracy: 0.8409\n",
      "Epoch 00222: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 329us/sample - loss: 1.4410 - categorical_accuracy: 0.8402 - val_loss: 1.7677 - val_categorical_accuracy: 0.6923\n",
      "Epoch 223/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3299 - categorical_accuracy: 0.8531\n",
      "Epoch 00223: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 252us/sample - loss: 1.4071 - categorical_accuracy: 0.8521 - val_loss: 1.7571 - val_categorical_accuracy: 0.6593\n",
      "Epoch 224/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5440 - categorical_accuracy: 0.8616\n",
      "Epoch 00224: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 308us/sample - loss: 1.4012 - categorical_accuracy: 0.8669 - val_loss: 1.7375 - val_categorical_accuracy: 0.7033\n",
      "Epoch 225/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.2150 - categorical_accuracy: 0.8711\n",
      "Epoch 00225: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 568us/sample - loss: 1.4197 - categorical_accuracy: 0.8580 - val_loss: 1.7455 - val_categorical_accuracy: 0.7033\n",
      "Epoch 226/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.5840 - categorical_accuracy: 0.8173\n",
      "Epoch 00226: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 488us/sample - loss: 1.4078 - categorical_accuracy: 0.8521 - val_loss: 1.7585 - val_categorical_accuracy: 0.6703\n",
      "Epoch 227/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4669 - categorical_accuracy: 0.8625\n",
      "Epoch 00227: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 403us/sample - loss: 1.4032 - categorical_accuracy: 0.8639 - val_loss: 1.7872 - val_categorical_accuracy: 0.6813\n",
      "Epoch 228/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3759 - categorical_accuracy: 0.8656\n",
      "Epoch 00228: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 228us/sample - loss: 1.4058 - categorical_accuracy: 0.8669 - val_loss: 1.7681 - val_categorical_accuracy: 0.6923\n",
      "Epoch 229/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4815 - categorical_accuracy: 0.8618\n",
      "Epoch 00229: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.3975 - categorical_accuracy: 0.8698 - val_loss: 1.8129 - val_categorical_accuracy: 0.6374\n",
      "Epoch 230/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3711 - categorical_accuracy: 0.8438\n",
      "Epoch 00230: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 577us/sample - loss: 1.4030 - categorical_accuracy: 0.8432 - val_loss: 1.7516 - val_categorical_accuracy: 0.6813\n",
      "Epoch 231/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.5549 - categorical_accuracy: 0.8566\n",
      "Epoch 00231: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.3973 - categorical_accuracy: 0.8639 - val_loss: 1.7557 - val_categorical_accuracy: 0.6813\n",
      "Epoch 232/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.5772 - categorical_accuracy: 0.8462\n",
      "Epoch 00232: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 337us/sample - loss: 1.3991 - categorical_accuracy: 0.8698 - val_loss: 1.7518 - val_categorical_accuracy: 0.6703\n",
      "Epoch 233/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.5733 - categorical_accuracy: 0.8555\n",
      "Epoch 00233: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 737us/sample - loss: 1.3994 - categorical_accuracy: 0.8669 - val_loss: 1.7621 - val_categorical_accuracy: 0.6813\n",
      "Epoch 234/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5636 - categorical_accuracy: 0.8393\n",
      "Epoch 00234: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 657us/sample - loss: 1.3965 - categorical_accuracy: 0.8580 - val_loss: 1.7778 - val_categorical_accuracy: 0.6813\n",
      "Epoch 235/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.4183 - categorical_accuracy: 0.8438\n",
      "Epoch 00235: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 373us/sample - loss: 1.3964 - categorical_accuracy: 0.8521 - val_loss: 1.7495 - val_categorical_accuracy: 0.6923\n",
      "Epoch 236/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3679 - categorical_accuracy: 0.8586\n",
      "Epoch 00236: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 225us/sample - loss: 1.3963 - categorical_accuracy: 0.8609 - val_loss: 1.7534 - val_categorical_accuracy: 0.6813\n",
      "Epoch 237/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4564 - categorical_accuracy: 0.8646\n",
      "Epoch 00237: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 249us/sample - loss: 1.3935 - categorical_accuracy: 0.8639 - val_loss: 1.7334 - val_categorical_accuracy: 0.7143\n",
      "Epoch 238/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3825 - categorical_accuracy: 0.8640\n",
      "Epoch 00238: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 243us/sample - loss: 1.4012 - categorical_accuracy: 0.8669 - val_loss: 1.7420 - val_categorical_accuracy: 0.7033\n",
      "Epoch 239/300\n",
      " 96/338 [=======>......................] - ETA: 0s - loss: 1.2407 - categorical_accuracy: 0.8750\n",
      "Epoch 00239: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 323us/sample - loss: 1.3950 - categorical_accuracy: 0.8728 - val_loss: 1.7472 - val_categorical_accuracy: 0.7143\n",
      "Epoch 240/300\n",
      "240/338 [====================>.........] - ETA: 0s - loss: 1.4486 - categorical_accuracy: 0.8625\n",
      "Epoch 00240: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 604us/sample - loss: 1.3919 - categorical_accuracy: 0.8669 - val_loss: 1.7628 - val_categorical_accuracy: 0.6813\n",
      "Epoch 241/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/338 [============================>.] - ETA: 0s - loss: 1.4061 - categorical_accuracy: 0.8631\n",
      "Epoch 00241: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 601us/sample - loss: 1.3985 - categorical_accuracy: 0.8639 - val_loss: 1.7537 - val_categorical_accuracy: 0.7033\n",
      "Epoch 242/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.4058 - categorical_accuracy: 0.8631\n",
      "Epoch 00242: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 426us/sample - loss: 1.3977 - categorical_accuracy: 0.8639 - val_loss: 1.7425 - val_categorical_accuracy: 0.6703\n",
      "Epoch 243/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3723 - categorical_accuracy: 0.8897\n",
      "Epoch 00243: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 388us/sample - loss: 1.3942 - categorical_accuracy: 0.8757 - val_loss: 1.7662 - val_categorical_accuracy: 0.6813\n",
      "Epoch 244/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4319 - categorical_accuracy: 0.8611\n",
      "Epoch 00244: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 240us/sample - loss: 1.3895 - categorical_accuracy: 0.8669 - val_loss: 1.7746 - val_categorical_accuracy: 0.6593\n",
      "Epoch 245/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2541 - categorical_accuracy: 0.8715\n",
      "Epoch 00245: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 231us/sample - loss: 1.3936 - categorical_accuracy: 0.8609 - val_loss: 1.7697 - val_categorical_accuracy: 0.6923\n",
      "Epoch 246/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.1440 - categorical_accuracy: 0.8750\n",
      "Epoch 00246: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.3958 - categorical_accuracy: 0.8639 - val_loss: 1.7509 - val_categorical_accuracy: 0.6923\n",
      "Epoch 247/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4596 - categorical_accuracy: 0.8500\n",
      "Epoch 00247: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 654us/sample - loss: 1.3939 - categorical_accuracy: 0.8550 - val_loss: 1.7418 - val_categorical_accuracy: 0.6923\n",
      "Epoch 248/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.3952 - categorical_accuracy: 0.8780\n",
      "Epoch 00248: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 240us/sample - loss: 1.3874 - categorical_accuracy: 0.8787 - val_loss: 1.7858 - val_categorical_accuracy: 0.6703\n",
      "Epoch 249/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4026 - categorical_accuracy: 0.8719\n",
      "Epoch 00249: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 716us/sample - loss: 1.3950 - categorical_accuracy: 0.8698 - val_loss: 1.7410 - val_categorical_accuracy: 0.7253\n",
      "Epoch 250/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3159 - categorical_accuracy: 0.8717\n",
      "Epoch 00250: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 672us/sample - loss: 1.3975 - categorical_accuracy: 0.8639 - val_loss: 1.7290 - val_categorical_accuracy: 0.7033\n",
      "Epoch 251/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3803 - categorical_accuracy: 0.8553\n",
      "Epoch 00251: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 361us/sample - loss: 1.4045 - categorical_accuracy: 0.8580 - val_loss: 1.7888 - val_categorical_accuracy: 0.6703\n",
      "Epoch 252/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5952 - categorical_accuracy: 0.8611\n",
      "Epoch 00252: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.4005 - categorical_accuracy: 0.8669 - val_loss: 1.7984 - val_categorical_accuracy: 0.6593\n",
      "Epoch 253/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.3202 - categorical_accuracy: 0.8684\n",
      "Epoch 00253: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.3985 - categorical_accuracy: 0.8639 - val_loss: 1.7729 - val_categorical_accuracy: 0.6703\n",
      "Epoch 254/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.1533 - categorical_accuracy: 0.8816\n",
      "Epoch 00254: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 222us/sample - loss: 1.3869 - categorical_accuracy: 0.8728 - val_loss: 1.7333 - val_categorical_accuracy: 0.6923\n",
      "Epoch 255/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.1762 - categorical_accuracy: 0.8906\n",
      "Epoch 00255: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 332us/sample - loss: 1.3897 - categorical_accuracy: 0.8669 - val_loss: 1.7530 - val_categorical_accuracy: 0.6923\n",
      "Epoch 256/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3931 - categorical_accuracy: 0.8750\n",
      "Epoch 00256: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 278us/sample - loss: 1.3881 - categorical_accuracy: 0.8669 - val_loss: 1.7525 - val_categorical_accuracy: 0.7033\n",
      "Epoch 257/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.4003 - categorical_accuracy: 0.8750\n",
      "Epoch 00257: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 379us/sample - loss: 1.3854 - categorical_accuracy: 0.8639 - val_loss: 1.7288 - val_categorical_accuracy: 0.7143\n",
      "Epoch 258/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.4896 - categorical_accuracy: 0.8654\n",
      "Epoch 00258: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 959us/sample - loss: 1.3961 - categorical_accuracy: 0.8609 - val_loss: 1.7351 - val_categorical_accuracy: 0.7033\n",
      "Epoch 259/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4012 - categorical_accuracy: 0.8882\n",
      "Epoch 00259: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 302us/sample - loss: 1.3877 - categorical_accuracy: 0.8817 - val_loss: 1.7690 - val_categorical_accuracy: 0.6923\n",
      "Epoch 260/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.2066 - categorical_accuracy: 0.8787\n",
      "Epoch 00260: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 252us/sample - loss: 1.3913 - categorical_accuracy: 0.8728 - val_loss: 1.7575 - val_categorical_accuracy: 0.6703\n",
      "Epoch 261/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4275 - categorical_accuracy: 0.8715\n",
      "Epoch 00261: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.3837 - categorical_accuracy: 0.8787 - val_loss: 1.7427 - val_categorical_accuracy: 0.7033\n",
      "Epoch 262/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.3155 - categorical_accuracy: 0.8611\n",
      "Epoch 00262: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.3830 - categorical_accuracy: 0.8609 - val_loss: 1.7260 - val_categorical_accuracy: 0.7033\n",
      "Epoch 263/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.5402 - categorical_accuracy: 0.8661\n",
      "Epoch 00263: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 358us/sample - loss: 1.3869 - categorical_accuracy: 0.8698 - val_loss: 1.7403 - val_categorical_accuracy: 0.6923\n",
      "Epoch 264/300\n",
      " 16/338 [>.............................] - ETA: 0s - loss: 3.2571 - categorical_accuracy: 0.7500\n",
      "Epoch 00264: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 184us/sample - loss: 1.3854 - categorical_accuracy: 0.8787 - val_loss: 1.8086 - val_categorical_accuracy: 0.6703\n",
      "Epoch 265/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.5835 - categorical_accuracy: 0.8359\n",
      "Epoch 00265: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 734us/sample - loss: 1.3934 - categorical_accuracy: 0.8550 - val_loss: 1.7408 - val_categorical_accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/300\n",
      "192/338 [================>.............] - ETA: 0s - loss: 1.5623 - categorical_accuracy: 0.8750\n",
      "Epoch 00266: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 613us/sample - loss: 1.3814 - categorical_accuracy: 0.8787 - val_loss: 1.7622 - val_categorical_accuracy: 0.6923\n",
      "Epoch 267/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3613 - categorical_accuracy: 0.8787\n",
      "Epoch 00267: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 648us/sample - loss: 1.3868 - categorical_accuracy: 0.8698 - val_loss: 1.7781 - val_categorical_accuracy: 0.6813\n",
      "Epoch 268/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.3126 - categorical_accuracy: 0.8516\n",
      "Epoch 00268: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 376us/sample - loss: 1.3855 - categorical_accuracy: 0.8550 - val_loss: 1.7472 - val_categorical_accuracy: 0.7033\n",
      "Epoch 269/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4470 - categorical_accuracy: 0.8719\n",
      "Epoch 00269: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 228us/sample - loss: 1.3883 - categorical_accuracy: 0.8669 - val_loss: 1.7294 - val_categorical_accuracy: 0.6923\n",
      "Epoch 270/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4474 - categorical_accuracy: 0.8875\n",
      "Epoch 00270: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 219us/sample - loss: 1.3914 - categorical_accuracy: 0.8846 - val_loss: 1.7499 - val_categorical_accuracy: 0.7253\n",
      "Epoch 271/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4480 - categorical_accuracy: 0.8656\n",
      "Epoch 00271: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 219us/sample - loss: 1.3849 - categorical_accuracy: 0.8698 - val_loss: 1.7354 - val_categorical_accuracy: 0.7143\n",
      "Epoch 272/300\n",
      "112/338 [========>.....................] - ETA: 0s - loss: 1.2559 - categorical_accuracy: 0.8661\n",
      "Epoch 00272: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 335us/sample - loss: 1.3809 - categorical_accuracy: 0.8698 - val_loss: 1.7744 - val_categorical_accuracy: 0.6923\n",
      "Epoch 273/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.3930 - categorical_accuracy: 0.8482\n",
      "Epoch 00273: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 281us/sample - loss: 1.3849 - categorical_accuracy: 0.8491 - val_loss: 1.7403 - val_categorical_accuracy: 0.7033\n",
      "Epoch 274/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.3915 - categorical_accuracy: 0.8690\n",
      "Epoch 00274: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 767us/sample - loss: 1.3860 - categorical_accuracy: 0.8698 - val_loss: 1.7579 - val_categorical_accuracy: 0.6813\n",
      "Epoch 275/300\n",
      "224/338 [==================>...........] - ETA: 0s - loss: 1.1663 - categorical_accuracy: 0.8750\n",
      "Epoch 00275: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 559us/sample - loss: 1.3849 - categorical_accuracy: 0.8550 - val_loss: 1.7630 - val_categorical_accuracy: 0.6703\n",
      "Epoch 276/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3520 - categorical_accuracy: 0.8640\n",
      "Epoch 00276: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 394us/sample - loss: 1.3816 - categorical_accuracy: 0.8580 - val_loss: 1.7336 - val_categorical_accuracy: 0.7143\n",
      "Epoch 277/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.1709 - categorical_accuracy: 0.8493\n",
      "Epoch 00277: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 252us/sample - loss: 1.4147 - categorical_accuracy: 0.8462 - val_loss: 1.7772 - val_categorical_accuracy: 0.6703\n",
      "Epoch 278/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.0542 - categorical_accuracy: 0.9044\n",
      "Epoch 00278: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 243us/sample - loss: 1.3786 - categorical_accuracy: 0.8698 - val_loss: 1.7939 - val_categorical_accuracy: 0.6703\n",
      "Epoch 279/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4201 - categorical_accuracy: 0.8421\n",
      "Epoch 00279: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 234us/sample - loss: 1.3941 - categorical_accuracy: 0.8491 - val_loss: 1.7242 - val_categorical_accuracy: 0.7143\n",
      "Epoch 280/300\n",
      "144/338 [===========>..................] - ETA: 0s - loss: 1.5942 - categorical_accuracy: 0.8472\n",
      "Epoch 00280: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 323us/sample - loss: 1.3788 - categorical_accuracy: 0.8757 - val_loss: 1.7332 - val_categorical_accuracy: 0.7253\n",
      "Epoch 281/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.3988 - categorical_accuracy: 0.8571\n",
      "Epoch 00281: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 420us/sample - loss: 1.3950 - categorical_accuracy: 0.8550 - val_loss: 1.7897 - val_categorical_accuracy: 0.6813\n",
      "Epoch 282/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5411 - categorical_accuracy: 0.8472\n",
      "Epoch 00282: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 441us/sample - loss: 1.4174 - categorical_accuracy: 0.8462 - val_loss: 1.7458 - val_categorical_accuracy: 0.6923\n",
      "Epoch 283/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.5352 - categorical_accuracy: 0.8566\n",
      "Epoch 00283: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 690us/sample - loss: 1.3770 - categorical_accuracy: 0.8728 - val_loss: 1.7718 - val_categorical_accuracy: 0.6813\n",
      "Epoch 284/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.3429 - categorical_accuracy: 0.8654\n",
      "Epoch 00284: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 278us/sample - loss: 1.3797 - categorical_accuracy: 0.8698 - val_loss: 1.7394 - val_categorical_accuracy: 0.7143\n",
      "Epoch 285/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.4107 - categorical_accuracy: 0.8611 ETA: 0s - loss: 1.3296 - categorical_accuracy: 0.87\n",
      "Epoch 00285: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 453us/sample - loss: 1.3764 - categorical_accuracy: 0.8698 - val_loss: 1.7329 - val_categorical_accuracy: 0.7143\n",
      "Epoch 286/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.2406 - categorical_accuracy: 0.8934\n",
      "Epoch 00286: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 255us/sample - loss: 1.3760 - categorical_accuracy: 0.8787 - val_loss: 1.7510 - val_categorical_accuracy: 0.7033\n",
      "Epoch 287/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.4802 - categorical_accuracy: 0.8640\n",
      "Epoch 00287: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 246us/sample - loss: 1.3758 - categorical_accuracy: 0.8728 - val_loss: 1.7344 - val_categorical_accuracy: 0.7143\n",
      "Epoch 288/300\n",
      "272/338 [=======================>......] - ETA: 0s - loss: 1.3628 - categorical_accuracy: 0.8713\n",
      "Epoch 00288: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 308us/sample - loss: 1.3756 - categorical_accuracy: 0.8698 - val_loss: 1.7898 - val_categorical_accuracy: 0.6703\n",
      "Epoch 289/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.3494 - categorical_accuracy: 0.8625\n",
      "Epoch 00289: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 213us/sample - loss: 1.3818 - categorical_accuracy: 0.8609 - val_loss: 1.7405 - val_categorical_accuracy: 0.7143\n",
      "Epoch 290/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/338 [===========================>..] - ETA: 0s - loss: 1.2940 - categorical_accuracy: 0.8687\n",
      "Epoch 00290: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 450us/sample - loss: 1.3761 - categorical_accuracy: 0.8669 - val_loss: 1.7305 - val_categorical_accuracy: 0.7033\n",
      "Epoch 291/300\n",
      "256/338 [=====================>........] - ETA: 0s - loss: 1.4206 - categorical_accuracy: 0.8750\n",
      "Epoch 00291: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 696us/sample - loss: 1.3732 - categorical_accuracy: 0.8757 - val_loss: 1.7296 - val_categorical_accuracy: 0.7033\n",
      "Epoch 292/300\n",
      "208/338 [=================>............] - ETA: 0s - loss: 1.2115 - categorical_accuracy: 0.8990 ETA: 0s - loss: 1.5175 - categorical_accuracy: 0.89\n",
      "Epoch 00292: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 483us/sample - loss: 1.3737 - categorical_accuracy: 0.8817 - val_loss: 1.7868 - val_categorical_accuracy: 0.6703\n",
      "Epoch 293/300\n",
      "176/338 [==============>...............] - ETA: 0s - loss: 1.3285 - categorical_accuracy: 0.8636\n",
      "Epoch 00293: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 376us/sample - loss: 1.3794 - categorical_accuracy: 0.8669 - val_loss: 1.7412 - val_categorical_accuracy: 0.7033\n",
      "Epoch 294/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2917 - categorical_accuracy: 0.8750\n",
      "Epoch 00294: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.3726 - categorical_accuracy: 0.8757 - val_loss: 1.7277 - val_categorical_accuracy: 0.7033\n",
      "Epoch 295/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.5290 - categorical_accuracy: 0.8542\n",
      "Epoch 00295: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 237us/sample - loss: 1.3725 - categorical_accuracy: 0.8728 - val_loss: 1.7661 - val_categorical_accuracy: 0.6813\n",
      "Epoch 296/300\n",
      "288/338 [========================>.....] - ETA: 0s - loss: 1.2343 - categorical_accuracy: 0.8819\n",
      "Epoch 00296: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 332us/sample - loss: 1.3730 - categorical_accuracy: 0.8669 - val_loss: 1.7660 - val_categorical_accuracy: 0.6703\n",
      "Epoch 297/300\n",
      "336/338 [============================>.] - ETA: 0s - loss: 1.3873 - categorical_accuracy: 0.8601\n",
      "Epoch 00297: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 204us/sample - loss: 1.3797 - categorical_accuracy: 0.8609 - val_loss: 1.7389 - val_categorical_accuracy: 0.7033\n",
      "Epoch 298/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4410 - categorical_accuracy: 0.8684\n",
      "Epoch 00298: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 639us/sample - loss: 1.3695 - categorical_accuracy: 0.8698 - val_loss: 1.7315 - val_categorical_accuracy: 0.7143\n",
      "Epoch 299/300\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 1.4939 - categorical_accuracy: 0.8750\n",
      "Epoch 00299: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 696us/sample - loss: 1.3683 - categorical_accuracy: 0.8817 - val_loss: 1.8302 - val_categorical_accuracy: 0.6813\n",
      "Epoch 300/300\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 1.4048 - categorical_accuracy: 0.8594\n",
      "Epoch 00300: val_categorical_accuracy did not improve from 0.73626\n",
      "338/338 [==============================] - 0s 734us/sample - loss: 1.3878 - categorical_accuracy: 0.8639 - val_loss: 1.7563 - val_categorical_accuracy: 0.6923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a411d50710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('..\\\\..\\\\Trained Models\\\\Transfered_Models\\\\Fully_Connected_Layers_Only\\\\'\n",
    "                                   +Target_data_name+'\\\\Fully_Connected_Layers_for_'+ Target_data_name +\n",
    "                                   '_With_source_data_as_'+ Source_data_name +'_overlap_ratio_' + \n",
    "                                   str(int(overlap_ratio * 100)) + '_percent.h5',\n",
    "                                   monitor='val_categorical_accuracy', verbose=1, save_best_only=True)\n",
    "model_1.compile(optimizer=keras.optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True),\n",
    "                loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain_transfer, Ytrain, epochs=300, batch_size=16, validation_data=(Xtest_transfer, Ytest), verbose=1,\n",
    "            callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 0s 957us/sample - loss: 1.7563 - categorical_accuracy: 0.6923\n",
      "Test Accuracy = 0.6923077\n"
     ]
    }
   ],
   "source": [
    "preds = model_1.evaluate(Xtest_transfer, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 0s 429us/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_1.predict(Xtest_transfer, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  2,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 15,  0,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  9,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  3,  0,  2,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  3,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9,  0,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  4,  3,  6,  7,  6, 16, 11,  6,  2,  4,  2,  2, 10,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import keras\n",
    "import sklearn.metrics\n",
    "import math\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding3D, BatchNormalization, Flatten, Conv3D, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_class(Class , Cube_size , Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    indx_class = np.where(Gt == Class)\n",
    "    all_indx = [[indx_class[0][i],indx_class[1][i]] for i in range(len(indx_class[0])) if len(Gt)-np.ceil(Cube_size/2)>indx_class[0][i]>np.ceil(Cube_size/2) and len(Gt[0])-np.ceil(Cube_size/2)>indx_class[1][i]>np.ceil(Cube_size/2)]\n",
    "    lst = []\n",
    "    small_segmented_1.append(np.array(Data[:ch, all_indx[0][0]-int(Cube_size/2):all_indx[0][0] + int(Cube_size/2), (all_indx[0][1]-int(Cube_size/2)):all_indx[0][1]+int(Cube_size/2)]))\n",
    "    small_seg_gt_1.append(Class)\n",
    "    lst.append([all_indx[0][0], all_indx[0][1]])\n",
    "    for i in range(1, len(all_indx)):\n",
    "        dist = []\n",
    "        for k in range(len(lst)):\n",
    "            d = math.sqrt((all_indx[i][0]-lst[k][0])**2 + (all_indx[i][1]-lst[k][1])**2)\n",
    "            dist.append(d)\n",
    "        if np.min(dist) > int(Cube_size*(1-overlap_ratio)):\n",
    "            small_segmented_1.append(np.array(Data[:ch, all_indx[i][0]-int(Cube_size/2):all_indx[i][0] + int(Cube_size/2), (all_indx[i][1]-int(Cube_size/2)):all_indx[i][1]+int(Cube_size/2)]))\n",
    "            small_seg_gt_1.append(Class)\n",
    "            lst.append([all_indx[i][0], all_indx[i][1]])\n",
    "    return small_segmented_1, small_seg_gt_1, lst\n",
    "\n",
    "def pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch):\n",
    "    class_len = []\n",
    "    for i in range_of_class:\n",
    "        \n",
    "        small_segmented_1, small_seg_gt_1, lst = pick_class(i, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch)\n",
    "        class_len.append(len(lst))\n",
    "    small_segmented_1 = np.array(small_segmented_1)\n",
    "    small_seg_gt_1 = np.array(small_seg_gt_1)\n",
    "    return small_segmented_1, small_seg_gt_1, class_len\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(percentage, class_len, Data, Gt):\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    Ytrain = []\n",
    "    Ytest = []\n",
    "    class_division = [0]\n",
    "    c = 0\n",
    "    for i in range(len(class_len)):\n",
    "        class_division.append(int(class_len[i]*(percentage/100)) + c)\n",
    "        class_division.append(class_len[i]+c)\n",
    "        c = class_len[i]+c\n",
    "    for i in range(1, len(class_division)):\n",
    "        if i%2!=0:\n",
    "            for j in range(class_division[i-1], class_division[i]):\n",
    "                Xtrain.append(Data[j])\n",
    "                Ytrain.append(Gt[j])\n",
    "        else:\n",
    "            for k in range(class_division[i-1], class_division[i]):\n",
    "                Xtest.append(Data[k])\n",
    "                Ytest.append(Gt[k])    \n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xtest = np.array(Xtest)\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    Ytest = np.array(Ytest)\n",
    "    s = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtrain = Xtrain[s]\n",
    "    Ytrain = Ytrain[s]\n",
    "    s = np.arange(Xtest.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Xtest = Xtest[s]\n",
    "    Ytest = Ytest[s]\n",
    "    Xtrain = np.expand_dims(Xtrain,axis=4) \n",
    "    Xtest = np.expand_dims(Xtest,axis=4) \n",
    "    Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape\n",
    "    values,counts = np.unique(Ytest, return_counts=True)\n",
    "    print(\"Total samples per class: \" + str(class_len) + \", Total number of samples is \" + str(np.sum(class_len))+'.\\n')\n",
    "    print(\"unique classes in Ytest: \" + str(values) + \", Total number of samples in Ytest is \" + str(np.sum(counts))+'.\\n'\n",
    "          +\"number of samples per class in Ytest: \" + str(counts) + '\\n')\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    Ytrain = Ytrain.reshape(len(Ytrain), 1)\n",
    "    Ytest = Ytest.reshape(len(Ytest),1)\n",
    "    Ytrain = onehot_encoder.fit_transform(Ytrain)\n",
    "    Ytest = onehot_encoder.fit_transform(Ytest)\n",
    "    \n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "def prepare_data_for_training(range_of_class, Cube_size, Data , Gt, small_segmented_1, small_seg_gt_1, percentage, overlap_ratio, ch):\n",
    "    small_segmented_1, small_seg_gt_1, class_len = pick_n_class(range_of_class, Cube_size, Data, Gt, small_segmented_1, small_seg_gt_1, overlap_ratio, ch)\n",
    "    Xtrain, Xtest, Ytrain, Ytest, class_len, counts = train_test_split(percentage, class_len, small_segmented_1, small_seg_gt_1)\n",
    "    return Xtrain, Xtest, Ytrain, Ytest, class_len, counts\n",
    "\n",
    "def SR_Unit(X, filters):\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    print(X.shape)\n",
    "    X = Conv3D(filters, (1, 1, 1), padding = \"same\")(X)\n",
    "    print('--------')\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization(axis = 4)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "\n",
    "    X = Conv3D(filters, kernel_size = (1, 3, 3), padding = 'same')(X)\n",
    "    X = BatchNormalization(axis = 4)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "\n",
    "\n",
    "    X = Conv3D(filters, kernel_size = (3, 1, 1), padding = \"same\")(X)\n",
    "    X = BatchNormalization(axis = 4)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    print(X.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    X = Conv3D(2*filters, kernel_size = (1, 1, 1), padding = \"same\")(X)\n",
    "    X = BatchNormalization(axis = 4)(X)\n",
    "    print(X.shape)\n",
    "\n",
    "    \n",
    "    X_shortcut = Conv3D(filters = 2*filters, kernel_size = (3, 3, 3), padding = 'same')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 4)(X_shortcut)\n",
    "\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    print(X.shape)\n",
    "    print('--------')\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "\n",
    "def feature_extraction(Sample):\n",
    "    X = Sample\n",
    "    X = Conv3D(32, (8, 3, 3), strides=(2,2,2), name='conv1')(X)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization(axis=4, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling3D((1, 2, 2), strides=None)(X)\n",
    "    print(X.shape)\n",
    "    # Stage 2\n",
    "    F1 = 16\n",
    "    X = SR_Unit(X, filters=F1)\n",
    "    print(X.shape)\n",
    "    F2 = 32\n",
    "    X = SR_Unit(X, filters=F2)\n",
    "    print(X.shape)\n",
    "#     F3 = 64\n",
    "#     X = SR_Unit(X, filters=F3)\n",
    "#     print(X.shape)\n",
    "#     F4 = 128\n",
    "#     X = SR_Unit(X, filters=F4)\n",
    "    print(X.shape)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def model(input_shape=(103, 50, 50, 1), classes=9):\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    print(X_input.shape)\n",
    "    X = feature_extraction(X_input)\n",
    "    \n",
    "    X = GlobalMaxPooling3D()(X)\n",
    "    print(X.shape)\n",
    "    X = Dense(256, input_dim=X.shape, activation='softmax', name='fc_256, kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, input_dim=X.shape, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name=\"3D-SRNet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pavia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU.mat')\n",
    "gt_uPavia = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\pavia\\\\PaviaU_gt.mat')\n",
    "data_PV = uPavia['paviaU']\n",
    "gt_PV = gt_uPavia['paviaU_gt']\n",
    "print(data_PV.shape, gt_PV.shape)\n",
    "data_PV = np.moveaxis(data_PV, 2, 0)\n",
    "print(data_PV.shape, gt_PV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,counts = np.unique(gt_PV, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class = range_of_class, Cube_size = 25, Data = data_PV , Gt = gt_PV, small_segmented_1 = [], small_seg_gt_1 = [], percentage = 80, overlap_ratio = 0.8, ch = 103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model(input_shape = Xtrain[0].shape, classes = len(range_of_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer= keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain, Ytrain, epochs = 50, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.layers[0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(input=model_1.layers[0].input, output=model_1.layers[35].output)\n",
    "model_new.save(\"model_pavia.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_pavia = load_model('model_pavia.h5')\n",
    "model_pavia.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_1.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_transfer = model_pavia.predict(Xtrain)\n",
    "Xtest_transfer = model_pavia.predict(Xtest)\n",
    "Xtrain_transfer = np.array(Xtrain_transfer)\n",
    "Xtest_transfer = np.array(Xtest_transfer)\n",
    "# Xtrain_transfer = np.expand_dims(Xtrain_transfer,axis=4)\n",
    "# Xtest_transfer = np.expand_dims(Xtest_transfer,axis=4)\n",
    "Xtest_transfer.shape, Xtrain_transfer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_transfer(input_shape=(64), classes=9):\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Dense(classes, input_dim=X_input.shape, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name=\"model_transfer\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = model_transfer(input_shape = Xtrain_transfer[0].shape, classes = len(range_of_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.compile(optimizer= keras.optimizers.SGD(lr=0.01), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_t.fit(Xtrain_transfer, Ytrain, epochs = 15, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_t.evaluate(Xtest_transfer, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model_t.predict(Xtest_transfer, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_t = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred_t, axis=1))\n",
    "confusion_matrix_t,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uSalinas = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Salinas\\\\Salinas_corrected.mat')\n",
    "gt_uSalinas = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Salinas\\\\Salinas_gt.mat')\n",
    "data_SA = uSalinas['salinas_corrected']\n",
    "gt_SA = gt_uSalinas['salinas_gt']\n",
    "print(data_SA.shape, gt_SA.shape)\n",
    "data_SA = np.moveaxis(data_SA, 2, 0)\n",
    "print(data_SA.shape, gt_SA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,counts = np.unique(gt_SA, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class = range_of_class, Cube_size = 25, Data = data_SA , Gt = gt_SA, small_segmented_1 = [], small_seg_gt_1 = [], percentage = 80, overlap_ratio = 0.8, ch=103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_transfer = model_pavia.predict(Xtrain)\n",
    "Xtest_transfer = model_pavia.predict(Xtest)\n",
    "Xtrain_transfer = np.array(Xtrain_transfer)\n",
    "Xtest_transfer = np.array(Xtest_transfer)\n",
    "# Xtrain_transfer = np.expand_dims(Xtrain_transfer,axis=4)\n",
    "# Xtest_transfer = np.expand_dims(Xtest_transfer,axis=4)\n",
    "Xtest_transfer.shape, Xtrain_transfer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = model_transfer(input_shape = Xtrain_transfer[0].shape, classes = len(range_of_class))\n",
    "model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.compile(optimizer= keras.optimizers.SGD(lr=0.01), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_t.fit(Xtrain_transfer, Ytrain, epochs = 15, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_t.evaluate(Xtest_transfer, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = model_t.predict(Xtest_transfer, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_t = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred_t, axis=1))\n",
    "for i in range(len(confusion_matrix_t)):\n",
    "    print(confusion_matrix_t[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model(input_shape = Xtrain[0].shape, classes = len(range_of_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer= keras.optimizers.SGD(lr=0.01), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain, Ytrain, epochs = 45, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_1.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian Pines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uIndian_pines = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Indian Pines\\\\Indian_pines_corrected.mat')\n",
    "gt_uIndian_pines = sio.loadmat('C:\\\\Users\\\\de991521\\\\Desktop\\\\EE_297_PROJECT\\\\Indian Pines\\\\Indian_pines_gt.mat')\n",
    "data_IN = uIndian_pines['indian_pines_corrected']\n",
    "gt_IN = gt_uIndian_pines['indian_pines_gt']\n",
    "print(data_IN.shape, gt_IN.shape)\n",
    "data_IN = np.moveaxis(data_IN, 2, 0)\n",
    "print(data_IN.shape, gt_IN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,counts = np.unique(gt_IN, return_counts=True)\n",
    "print(values,counts)\n",
    "range_of_class = list(values)\n",
    "if 0 in range_of_class:\n",
    "    range_of_class.pop(0)\n",
    "range_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest, class_len, counts = prepare_data_for_training(range_of_class = range_of_class, Cube_size = 10, Data = data_IN , Gt = gt_IN, small_segmented_1 = [], small_seg_gt_1 = [], percentage = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model(input_shape = Xtrain[0].shape, classes = len(range_of_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer= keras.optimizers.SGD(lr=0.01), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_1.fit(Xtrain, Ytrain, epochs = 45, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_1.evaluate(Xtest, Ytest)\n",
    "# print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(Ytest, axis=1), np.argmax(y_pred, axis=1))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
